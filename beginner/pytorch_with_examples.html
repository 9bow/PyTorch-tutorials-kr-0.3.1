

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>예제로 배우는 PyTorch &mdash; PyTorch Tutorials 0.3.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  
    <link rel="stylesheet" href="../_static/css/pytorch_theme.css" type="text/css" />
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="PyTorch Tutorials 0.3.1 documentation" href="../index.html"/>
        <link rel="next" title="준비 운동: NumPy" href="examples_tensor/two_layer_net_numpy.html"/>
        <link rel="prev" title="Multi-GPU examples" href="former_torchies/parallelism_tutorial.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PyTorch Tutorials
          

          
            
            <img src="../_static/pytorch-logo-dark.svg" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                0.3.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Beginner Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="deep_learning_60min_blitz.html">PyTorch로 딥러닝하기: 60분만에 끝장내기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="blitz/tensor_tutorial.html">PyTorch가 무엇인가요?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="blitz/tensor_tutorial.html#id1">시작하기</a><ul>
<li class="toctree-l4"><a class="reference internal" href="blitz/tensor_tutorial.html#tensors">Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="blitz/tensor_tutorial.html#operations">연산(Operations)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="blitz/tensor_tutorial.html#numpy-bridge">NumPy 변환(Bridge)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="blitz/tensor_tutorial.html#torch-tensor-numpy">Torch Tensor를 NumPy 배열로 변환하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="blitz/tensor_tutorial.html#numpy-torch-tensor">NumPy 배열을 Torch Tensor로 변환하기</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="blitz/tensor_tutorial.html#cuda-tensors">CUDA Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="blitz/autograd_tutorial.html">Autograd: 자동 미분</a><ul>
<li class="toctree-l3"><a class="reference internal" href="blitz/autograd_tutorial.html#variable">변수(Variable)</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/autograd_tutorial.html#gradient">변화도(Gradient)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="blitz/neural_networks_tutorial.html">신경망(Neural Networks)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="blitz/neural_networks_tutorial.html#id1">신경망 정의하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/neural_networks_tutorial.html#loss-function">손실 함수 (Loss Function)</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/neural_networks_tutorial.html#backprop">역전파(Backprop)</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/neural_networks_tutorial.html#id4">가중치 갱신</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="blitz/cifar10_tutorial.html">분류기(Classifier) 학습하기</a><ul>
<li class="toctree-l3"><a class="reference internal" href="blitz/cifar10_tutorial.html#id1">데이터는 어떻게 하나요?</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/cifar10_tutorial.html#id2">이미지 분류기 학습하기</a><ul>
<li class="toctree-l4"><a class="reference internal" href="blitz/cifar10_tutorial.html#cifar10">1. CIFAR10를 불러오고 정규화하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="blitz/cifar10_tutorial.html#convolution-neural-network">2. 합성곱 신경망(Convolution Neural Network) 정의하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="blitz/cifar10_tutorial.html#optimizer">3. 손실 함수와 Optimizer 정의하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="blitz/cifar10_tutorial.html#id3">4. 신경망 학습하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="blitz/cifar10_tutorial.html#id4">5. 시험용 데이터로 신경망 검사하기</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="blitz/cifar10_tutorial.html#gpu">GPU에서 학습하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/cifar10_tutorial.html#id5">여러개의 GPU에서 학습하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/cifar10_tutorial.html#id6">이제 뭘 해볼까요?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="blitz/data_parallel_tutorial.html">Optional: Data Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="blitz/data_parallel_tutorial.html#imports-and-parameters">Imports and parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/data_parallel_tutorial.html#dummy-dataset">Dummy DataSet</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/data_parallel_tutorial.html#simple-model">Simple Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/data_parallel_tutorial.html#create-model-and-dataparallel">Create Model and DataParallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/data_parallel_tutorial.html#run-the-model">Run the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="blitz/data_parallel_tutorial.html#results">Results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="blitz/data_parallel_tutorial.html#gpus">2 GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="blitz/data_parallel_tutorial.html#id1">3 GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="blitz/data_parallel_tutorial.html#id2">8 GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="blitz/data_parallel_tutorial.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="former_torchies_tutorial.html">Torch 사용자를 위한 PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="former_torchies/tensor_tutorial.html">Tensor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/tensor_tutorial.html#in-place-out-of-place">In-place / Out-of-place</a></li>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/tensor_tutorial.html#id1">0-인덱스</a></li>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/tensor_tutorial.html#camel-case">카멜표기법(Camel Case) 없음</a></li>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/tensor_tutorial.html#numpy-bridge">NumPy 변환(Bridge)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="former_torchies/tensor_tutorial.html#torch-tensor-numpy">Torch Tensor를 NumPy 배열로 변환하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="former_torchies/tensor_tutorial.html#numpy-torch-tensor">NumPy 배열을 Torch Tensor로 변환하기</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/tensor_tutorial.html#cuda-tensors">CUDA Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="former_torchies/autograd_tutorial.html">Autograd</a><ul>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/autograd_tutorial.html#variable">변수(Variable)</a></li>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/autograd_tutorial.html#gradient">변화도(Gradient)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="former_torchies/nn_tutorial.html">nn 패키지</a><ul>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/nn_tutorial.html#convnet">예제1: 합성곱 신경망(ConvNet)</a></li>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/nn_tutorial.html#hook">순방향/역방향 함수 훅(Hook)</a></li>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/nn_tutorial.html#recurrent-nets">예제2: 순환 신경망(Recurrent Nets)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="former_torchies/parallelism_tutorial.html">Multi-GPU examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/parallelism_tutorial.html#dataparallel">DataParallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="former_torchies/parallelism_tutorial.html#part-of-the-model-on-cpu-and-part-on-the-gpu">Part of the model on CPU and part on the GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">예제로 배우는 PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#tensor">Tensor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#numpy">준비 운동: NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-tensor">PyTorch: Tensor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#autograd">Autograd</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-variables-autograd">PyTorch: Variables과 autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-autograd">PyTorch: 새 autograd 함수 정의하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tensorflow-static-graph">TensorFlow: 정적 그래프(Static Graph)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#nn"><cite>nn</cite> 모듈</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-nn">PyTorch: nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-optim">PyTorch: optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-custom-nn-modules">PyTorch: Custom nn Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-control-flow-weight-sharing">PyTorch: Control Flow + Weight Sharing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#examples">Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tensors">Tensors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="examples_tensor/two_layer_net_numpy.html">준비 운동: NumPy</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples_tensor/two_layer_net_tensor.html">PyTorch: Tensor</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id3">Autograd</a><ul>
<li class="toctree-l4"><a class="reference internal" href="examples_autograd/two_layer_net_autograd.html">PyTorch: Variable과 autograd</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples_autograd/two_layer_net_custom_function.html">PyTorch: 새 autograd 함수 정의하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples_autograd/tf_two_layer_net.html">TensorFlow: 정적 그래프(Static Graph)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#nn-module"><cite>nn</cite> module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="examples_nn/two_layer_net_nn.html">PyTorch: nn</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples_nn/two_layer_net_optim.html">PyTorch: optim</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples_nn/two_layer_net_module.html">PyTorch: 사용자 정의 nn 모듈</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples_nn/dynamic_net.html">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="transfer_learning_tutorial.html">Transfer Learning tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="transfer_learning_tutorial.html#load-data">Load Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transfer_learning_tutorial.html#visualize-a-few-images">Visualize a few images</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transfer_learning_tutorial.html#training-the-model">Training the model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transfer_learning_tutorial.html#visualizing-the-model-predictions">Visualizing the model predictions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transfer_learning_tutorial.html#finetuning-the-convnet">Finetuning the convnet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transfer_learning_tutorial.html#train-and-evaluate">Train and evaluate</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transfer_learning_tutorial.html#convnet-as-fixed-feature-extractor">ConvNet as fixed feature extractor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="transfer_learning_tutorial.html#id1">Train and evaluate</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_loading_tutorial.html">Data Loading and Processing Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="data_loading_tutorial.html#dataset-class">Dataset class</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_loading_tutorial.html#transforms">Transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="data_loading_tutorial.html#compose-transforms">Compose transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="data_loading_tutorial.html#iterating-through-the-dataset">Iterating through the dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_loading_tutorial.html#afterword-torchvision">Afterword: torchvision</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="deep_learning_nlp_tutorial.html">Deep Learning for NLP with Pytorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nlp/pytorch_tutorial.html">Introduction to PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nlp/pytorch_tutorial.html#introduction-to-torch-s-tensor-library">Introduction to Torch’s tensor library</a><ul>
<li class="toctree-l4"><a class="reference internal" href="nlp/pytorch_tutorial.html#creating-tensors">Creating Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="nlp/pytorch_tutorial.html#operations-with-tensors">Operations with Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="nlp/pytorch_tutorial.html#reshaping-tensors">Reshaping Tensors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="nlp/pytorch_tutorial.html#computation-graphs-and-automatic-differentiation">Computation Graphs and Automatic Differentiation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nlp/deep_learning_tutorial.html">Deep Learning with PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nlp/deep_learning_tutorial.html#deep-learning-building-blocks-affine-maps-non-linearities-and-objectives">Deep Learning Building Blocks: Affine maps, non-linearities and objectives</a><ul>
<li class="toctree-l4"><a class="reference internal" href="nlp/deep_learning_tutorial.html#affine-maps">Affine Maps</a></li>
<li class="toctree-l4"><a class="reference internal" href="nlp/deep_learning_tutorial.html#non-linearities">Non-Linearities</a></li>
<li class="toctree-l4"><a class="reference internal" href="nlp/deep_learning_tutorial.html#softmax-and-probabilities">Softmax and Probabilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="nlp/deep_learning_tutorial.html#objective-functions">Objective Functions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="nlp/deep_learning_tutorial.html#optimization-and-training">Optimization and Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp/deep_learning_tutorial.html#creating-network-components-in-pytorch">Creating Network Components in Pytorch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="nlp/deep_learning_tutorial.html#example-logistic-regression-bag-of-words-classifier">Example: Logistic Regression Bag-of-Words classifier</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nlp/word_embeddings_tutorial.html">Word Embeddings: Encoding Lexical Semantics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nlp/word_embeddings_tutorial.html#getting-dense-word-embeddings">Getting Dense Word Embeddings</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp/word_embeddings_tutorial.html#word-embeddings-in-pytorch">Word Embeddings in Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp/word_embeddings_tutorial.html#an-example-n-gram-language-modeling">An Example: N-Gram Language Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words">Exercise: Computing Word Embeddings: Continuous Bag-of-Words</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nlp/sequence_models_tutorial.html">Sequence Models and Long-Short Term Memory Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nlp/sequence_models_tutorial.html#lstm-s-in-pytorch">LSTM’s in Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp/sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging">Example: An LSTM for Part-of-Speech Tagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp/sequence_models_tutorial.html#exercise-augmenting-the-lstm-part-of-speech-tagger-with-character-level-features">Exercise: Augmenting the LSTM part-of-speech tagger with character-level features</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nlp/advanced_tutorial.html">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nlp/advanced_tutorial.html#dynamic-versus-static-deep-learning-toolkits">Dynamic versus Static Deep Learning Toolkits</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp/advanced_tutorial.html#bi-lstm-conditional-random-field-discussion">Bi-LSTM Conditional Random Field Discussion</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp/advanced_tutorial.html#implementation-notes">Implementation Notes</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp/advanced_tutorial.html#exercise-a-new-loss-function-for-discriminative-tagging">Exercise: A new loss function for discriminative tagging</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Intermediate Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html">Classifying Names with a Character-Level RNN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#preparing-the-data">Preparing the Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#turning-names-into-tensors">Turning Names into Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#creating-the-network">Creating the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#preparing-for-training">Preparing for Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#training-the-network">Training the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#plotting-the-results">Plotting the Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#evaluating-the-results">Evaluating the Results</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#running-on-user-input">Running on User Input</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html">Generating Names with a Character-Level RNN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#preparing-the-data">Preparing the Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#creating-the-network">Creating the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#preparing-for-training">Preparing for Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#training-the-network">Training the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#plotting-the-losses">Plotting the Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#sampling-the-network">Sampling the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html">Translation with a Sequence to Sequence Network and Attention</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#loading-data-files">Loading data files</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model">The Seq2Seq Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#the-encoder">The Encoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#the-decoder">The Decoder</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#simple-decoder">Simple Decoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#attention-decoder">Attention Decoder</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#preparing-training-data">Preparing Training Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#training-the-model">Training the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#plotting-results">Plotting results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#evaluation">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#training-and-evaluating">Training and Evaluating</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#visualizing-attention">Visualizing Attention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html">Reinforcement Learning (DQN) tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#replay-memory">Replay Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#dqn-algorithm">DQN algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#q-network">Q-network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#input-extraction">Input extraction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#hyperparameters-and-utilities">Hyperparameters and utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#training-loop">Training loop</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dist_tuto.html">파이토치로 분산 어플리케이션 개발하기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/dist_tuto.html#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/dist_tuto.html#point-to-point-communication">Point-to-Point Communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/dist_tuto.html#collective-communication">Collective Communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/dist_tuto.html#distributed-training">Distributed Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/dist_tuto.html#our-own-ring-allreduce">Our Own Ring-Allreduce</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/dist_tuto.html#advanced-topics">Advanced Topics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/dist_tuto.html#communication-backends">Communication Backends</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/dist_tuto.html#initialization-methods">Initialization Methods</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html#loading-the-data">Loading the data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html#depicting-spatial-transformer-networks">Depicting spatial transformer networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html#training-the-model">Training the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html#visualizing-the-stn-results">Visualizing the STN results</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Advanced Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/neural_style_tutorial.html">Neural Transfer with PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#introduction">Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#neural-what">Neural what?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#how-does-it-work">How does it work?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../advanced/neural_style_tutorial.html#ok-how-does-it-work">OK. How does it work?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#pytorch-implementation">PyTorch implementation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#packages">Packages</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#cuda">Cuda</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#load-images">Load images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#display-images">Display images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#content-loss">Content loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#style-loss">Style loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#load-the-neural-network">Load the neural network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#input-image">Input image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#gradient-descent">Gradient descent</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html">Creating extensions using numpy and scipy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html#parameter-less-example">Parameter-less example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html#parametrized-example">Parametrized example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/super_resolution_with_caffe2.html">Transfering a model from PyTorch to Caffe2 and Mobile using ONNX</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/super_resolution_with_caffe2.html#transfering-srresnet-using-onnx">Transfering SRResNet using ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/super_resolution_with_caffe2.html#running-the-model-on-mobile-devices">Running the model on mobile devices</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/c_extension.html">C 언어로 PyTorch 확장 기능(custom extension) 만들기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/c_extension.html#c">1단계. C 코드 준비하기</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/c_extension.html#python">2단계. Python에서 불러오기</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PyTorch Tutorials</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>예제로 배우는 PyTorch</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/beginner/pytorch_with_examples.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="pytorch">
<h1>예제로 배우는 PyTorch<a class="headerlink" href="#pytorch" title="Permalink to this headline">¶</a></h1>
<dl class="docutils">
<dt><strong>Author</strong>: <a class="reference external" href="https://github.com/jcjohnson/pytorch-examples">Justin Johnson</a></dt>
<dd><strong>번역</strong>: <a class="reference external" href="http://github.com/9bow">박정환</a></dd>
</dl>
<p>이 튜토리얼에서는 <a class="reference external" href="https://github.com/pytorch/pytorch">PyTorch</a> 의 기본적인
개념을 포함된 예제를 통해 소개합니다.</p>
<p>PyTorch의 핵심에는 2가지 주요한 특징이 있습니다:</p>
<ul class="simple">
<li>NumPy와 유사하지만 GPU 상에서 실행 가능한 N차원 Tensor</li>
<li>신경망을 구성하고 학습하는 과정에서의 자동 미분</li>
</ul>
<p>완전히 연결된 ReLU 신경망을 예제로 사용할 것입니다. 이 신경망은 하나의 은닉
계층(Hidden Layer)을 갖고 있으며, 신경망의 출력과 정답 사이의 유클리드 거리
(Euclidean Distance)를 최소화하는 식으로 경사하강법(Gradient Descent)을 사용하여
무작위의 데이터를 맞추도록 학습할 것입니다.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">각각의 에제들은 <a class="reference internal" href="#examples-download"><span class="std std-ref">이 페이지의 마지막 부분</span></a> 에서
살펴볼 수 있습니다.</p>
</div>
<div class="contents local topic" id="id2">
<p class="topic-title first">목차</p>
<ul class="simple">
<li><a class="reference internal" href="#tensor" id="id13">Tensor</a><ul>
<li><a class="reference internal" href="#numpy" id="id14">준비 운동: NumPy</a></li>
<li><a class="reference internal" href="#pytorch-tensor" id="id15">PyTorch: Tensor</a></li>
</ul>
</li>
<li><a class="reference internal" href="#autograd" id="id16">Autograd</a><ul>
<li><a class="reference internal" href="#pytorch-variables-autograd" id="id17">PyTorch: Variables과 autograd</a></li>
<li><a class="reference internal" href="#pytorch-autograd" id="id18">PyTorch: 새 autograd 함수 정의하기</a></li>
<li><a class="reference internal" href="#tensorflow-static-graph" id="id19">TensorFlow: 정적 그래프(Static Graph)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nn" id="id20"><cite>nn</cite> 모듈</a><ul>
<li><a class="reference internal" href="#pytorch-nn" id="id21">PyTorch: nn</a></li>
<li><a class="reference internal" href="#pytorch-optim" id="id22">PyTorch: optim</a></li>
<li><a class="reference internal" href="#pytorch-custom-nn-modules" id="id23">PyTorch: Custom nn Modules</a></li>
<li><a class="reference internal" href="#pytorch-control-flow-weight-sharing" id="id24">PyTorch: Control Flow + Weight Sharing</a></li>
</ul>
</li>
<li><a class="reference internal" href="#examples" id="id25">Examples</a><ul>
<li><a class="reference internal" href="#tensors" id="id26">Tensors</a></li>
<li><a class="reference internal" href="#id3" id="id27">Autograd</a></li>
<li><a class="reference internal" href="#nn-module" id="id28"><cite>nn</cite> module</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="tensor">
<h2><a class="toc-backref" href="#id13">Tensor</a><a class="headerlink" href="#tensor" title="Permalink to this headline">¶</a></h2>
<div class="section" id="numpy">
<h3><a class="toc-backref" href="#id14">준비 운동: NumPy</a><a class="headerlink" href="#numpy" title="Permalink to this headline">¶</a></h3>
<p>PyTorch를 소개하기 전에, 먼저 NumPy를 사용하여 신경망을 구성해보겠습니다.</p>
<p>NumPy는 N차원 배열 객체(Object)를 제공하며, 이러한 배열을 조작하기 위한 다양한
함수(function)들을 제공합니다. NumPy는 과학적 분야의 연산을 위한 포괄적인 프레임워크
(Framework)입니다; 이는 연산 그래프(Computational Graph)나 딥러닝, 변화도(Gradient)에
대해서는 알지 못합니다. 하지만 NumPy 연산을 사용하여 순전파 단계와 역전파 단계를
직접 구현함으로써, 2-계층을 갖는 신경망이 무작위의 데이터를 맞추도록 할 수
있습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># N은 배치 크기이며, D_in은 입력의 차원입니다;</span>
<span class="c1"># H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># 무작위의 입력과 출력 데이터를 생성합니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

<span class="c1"># 무작위로 가중치를 초기화합니다.</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 예측값 y를 계산합니다.</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span>
    <span class="n">h_relu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">h_relu</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>

    <span class="c1"># 손실(loss)을 계산하고 출력합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

    <span class="c1"># 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.</span>
    <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">h_relu</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_y_pred</span><span class="p">)</span>
    <span class="n">grad_h_relu</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">grad_h</span> <span class="o">=</span> <span class="n">grad_h_relu</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">grad_h</span><span class="p">[</span><span class="n">h</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">grad_w1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_h</span><span class="p">)</span>

    <span class="c1"># 가중치를 갱신합니다.</span>
    <span class="n">w1</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1</span>
    <span class="n">w2</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2</span>
</pre></div>
</div>
</div>
<div class="section" id="pytorch-tensor">
<h3><a class="toc-backref" href="#id15">PyTorch: Tensor</a><a class="headerlink" href="#pytorch-tensor" title="Permalink to this headline">¶</a></h3>
<p>NumPy는 훌륭한 프레임워크지만, GPU를 사용하여 수치 연산을 가속화할 수는 없습니다.
현대의 심층 신경망에서 GPU는 종종 <a class="reference external" href="https://github.com/jcjohnson/cnn-benchmarks">50배 또는 그 이상</a>
의 속도 향상을 제공하기 때문에, 안타깝게도 NumPy는 현대의 딥러닝에는 충분치 않습니다.</p>
<p>이번에는 PyTorch의 기본적인 개념인 <strong>Tensor</strong> 에 대해서 소개하겠습니다.
PyTorch Tensor는 기본적으로 NumPy 배열과 동일합니다: Tensor는 N차원 배열이며,
PyTorch는 Tensor 연산을 위한 다양한 함수들을 제공합니다. NumPy 배열과 같이,
PyTorch Tensor는 딥러닝이나 연산 그래프, 변화도는 알지 못하며, 과학적 분야의 연산을
위한 포괄적인 도구입니다.</p>
<p>그러나 NumPy와는 달리, PyTorch Tensor는 GPU를 활용하여 수치 연산을 가속화할 수
있습니다. GPU에서 PyTorch Tensor를 실행하기 위해서는 단지 새로운 자료형으로
변환(Cast)해주기만 하면 됩니다.</p>
<p>여기에서는 PyTorch Tensor를 사용하여 2-계층의 신경망이 무작위 데이터를 맞추도록
할 것입니다. 위의 NumPy 예제에서와 같이 신경망의 순전파 단계와 역전파 단계는 직접
구현하겠습니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="kn">import</span> <span class="nn">torch</span>


<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span>
<span class="c1"># dtype = torch.cuda.FloatTensor # GPU에서 실행하려면 이 주석을 제거하세요.</span>

<span class="c1"># N은 배치 크기이며, D_in은 입력의 차원입니다;</span>
<span class="c1"># H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># 무작위의 입력과 출력 데이터를 생성합니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># 무작위로 가중치를 초기화합니다.</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 예측값 y를 계산합니다.</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span>
    <span class="n">h_relu</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">h_relu</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>

    <span class="c1"># 손실(loss)을 계산하고 출력합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

    <span class="c1"># 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.</span>
    <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">h_relu</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">grad_y_pred</span><span class="p">)</span>
    <span class="n">grad_h_relu</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w2</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="n">grad_h</span> <span class="o">=</span> <span class="n">grad_h_relu</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">grad_h</span><span class="p">[</span><span class="n">h</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">grad_w1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">grad_h</span><span class="p">)</span>

    <span class="c1"># 경사하강법(Gradient Descent)를 사용하여 가중치를 갱신합니다.</span>
    <span class="n">w1</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1</span>
    <span class="n">w2</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="autograd">
<h2><a class="toc-backref" href="#id16">Autograd</a><a class="headerlink" href="#autograd" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pytorch-variables-autograd">
<h3><a class="toc-backref" href="#id17">PyTorch: Variables과 autograd</a><a class="headerlink" href="#pytorch-variables-autograd" title="Permalink to this headline">¶</a></h3>
<p>위의 예제에서 우리는 신경망의 순전파 단계와 역전파 단계를 수동으로 구현하였습니다.
작은 2-계층 신경망에서 역전파 단계를 직접 구현하는 것은 큰 일이 아니지만,
대규모의 복잡한 신경망에서는 매우 아슬아슬한 일일 것입니다.</p>
<p>다행히도, <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">자동 미분</a> 을
사용하여 신경망에서 역전파 단계의 연산을 자동화할 수 있습니다. PyTorch의 <strong>autograd</strong>
패키지는 이 기능을 정확히 제공합니다.
Autograd를 사용할 때, 신경망의 순전파 단계는 <strong>연산 그래프</strong> 를 정의합니다;
그래프의 노드(Node)는 Tensor이며, 엣지(Edge)는 입력 Tensor로부터 출력 Tensor를
만들어내는 함수입니다. 이 그래프를 통해 역전파를 하게 되면 변화도를 쉽게 계산할
수 있습니다.</p>
<p>이는 복잡해보이지만 실제로 사용하는 것은 간단합니다. PyTorch Tensor를 <strong>Variable</strong>
객체로 감싸게 되면, 이 Variable이 연산 그래프에서 노드로 표현(represent)됩니다.
<code class="docutils literal notranslate"><span class="pre">x</span></code> 가 Variable일 때, <code class="docutils literal notranslate"><span class="pre">x.data</span></code> 는 그 값을 갖는 Tensor이며 <code class="docutils literal notranslate"><span class="pre">x.grad</span></code> 는 어떤
스칼라 값에 대해 <code class="docutils literal notranslate"><span class="pre">x</span></code> 에 대한 변화도를 갖는 또 다른 Variable 입니다.</p>
<p>PyTorch Variable은 PyTorch Tensor와 동일한 API를 제공합니다: Tensor에서 할 수 있는
(거의) 모든 연산은 Variable에서도 할 수 있습니다; 차이점은 연산 그래프를 정의할 때
Variable을 사용하면, 자동으로 변화도를 계산할 수 있다는 것입니다.</p>
<p>여기에서는 PyTorch Variable과 autograd를 사용하여 2-계층 신경망을 구현합니다; 이제
더 이상 신경망의 역전파 단계를 직접 구현할 필요가 없습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span>
<span class="c1"># dtype = torch.cuda.FloatTensor # GPU에서 실행하려면 이 주석을 제거하세요.</span>

<span class="c1"># N은 배치 크기이며, D_in은 입력의 차원입니다;</span>
<span class="c1"># H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로</span>
<span class="c1"># 감쌉니다. requires_grade=False로 설정하여 역전파 중에 이 Variable들에 대한</span>
<span class="c1"># 변화도를 계산할 필요가 없음을 나타냅니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로</span>
<span class="c1"># 감쌉니다. requires_grad=True로 설정하여 역전파 중에 이 Variable들에 대한</span>
<span class="c1"># 변화도를 계산할 필요가 있음을 나타냅니다.</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: Variable 연산을 사용하여 y 값을 예측합니다. 이는 Tensor를 사용한</span>
    <span class="c1"># 순전파 단계와 완전히 동일하지만, 역전파 단계를 별도로 구현하지 않기 위해 중간</span>
    <span class="c1"># 값들(Intermediate Value)에 대한 참조(Reference)를 갖고 있을 필요가 없습니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>

    <span class="c1"># Variable 연산을 사용하여 손실을 계산하고 출력합니다.</span>
    <span class="c1"># loss는 (1,) 모양을 갖는 Variable이며, loss.data는 (1,) 모양의 Tensor입니다;</span>
    <span class="c1"># loss.data[0]은 손실(loss)의 스칼라 값입니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># autograde를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를</span>
    <span class="c1"># 갖는 모든 Variable에 대한 손실의 변화도를 계산합니다. 이후 w1.grad와 w2.grad는</span>
    <span class="c1"># w1과 w2 각각에 대한 손실의 변화도를 갖는 Variable이 됩니다.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 경사하강법(Gradient Descent)을 사용하여 가중치를 갱신합니다; w1.data와</span>
    <span class="c1"># w2.data는 Tensor이며, w1.grad와 w2.grad는 Variable이고, w1.grad.data와</span>
    <span class="c1"># w2.grad.data는 Tensor입니다.</span>
    <span class="n">w1</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
    <span class="n">w2</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">w2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>

    <span class="c1"># 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.</span>
    <span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">w2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="pytorch-autograd">
<h3><a class="toc-backref" href="#id18">PyTorch: 새 autograd 함수 정의하기</a><a class="headerlink" href="#pytorch-autograd" title="Permalink to this headline">¶</a></h3>
<p>Under the hood, autograd의 기본(primitive) 연산자는 실제로 Tensor를 조작하는 2개의
함수입니다. <strong>forward</strong> 함수는 입력 Tensor로부터 출력 Tensor를 계산합니다.
<strong>backward</strong> 함수는 출력 Tensor의 변화도를 받고 입력 Tensor의 변화도를 계산합니다.</p>
<p>PyTorch에서 <code class="docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> 의 하위 클래스(subclass)를 정의하고
<code class="docutils literal notranslate"><span class="pre">forward</span></code> 와 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 함수를 구현함으로써 쉽게 사용자 정의 autograd 연산자를
정의할 수 있습니다. 그 후, 인스턴스(instance)를 생성하고 함수처럼 호출하여
입력 데이터를 포함하는 Variable을 전달하는 식으로 새로운 autograd 연산자를 쉽게
사용할 수 있습니다.</p>
<p>이 예제에서는 ReLU 비선형성(nonlinearity)을 수행하기 위한 사용자 정의 autograd
함수를 정의하고, 2-계층 신경망에 이를 적용해보도록 하겠습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>


<span class="k">class</span> <span class="nc">MyReLU</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    torch.autograd.Function을 상속받아 사용자 정의 autograd 함수를 구현하고,</span>
<span class="sd">    Tensor 연산을 하는 순전파와 역전파 단계를 구현하겠습니다.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        순전파 단계에서는 입력을 갖는 Tensor를 받아 출력을 갖는 Tensor를 반환합니다.</span>
<span class="sd">        ctx는 역전파 연산을 위한 정보를 저장하기 위해 사용하는 Context Object입니다.</span>
<span class="sd">        ctx.save_for_backward method를 사용하여 역전파 단계에서 사용할 어떠한</span>
<span class="sd">        객체(object)도 저장(cache)해 둘 수 있습니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        역전파 단계에서는 출력에 대한 손실의 변화도를 갖는 Tensor를 받고, 입력에</span>
<span class="sd">        대한 손실의 변화도를 계산합니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">input</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">grad_input</span><span class="p">[</span><span class="nb">input</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">grad_input</span>


<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span>
<span class="c1"># dtype = torch.cuda.FloatTensor # GPU에서 실행하려면 이 주석을 제거하세요.</span>

<span class="c1"># N은 배치 크기이며, D_in은 입력의 차원입니다;</span>
<span class="c1"># H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로</span>
<span class="c1"># 감쌉니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로</span>
<span class="c1"># 감쌉니다.</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># 사용자 정의 함수를 적용하기 위해 Function.apply method를 사용합니다.</span>
    <span class="c1"># 이를 &#39;relu&#39;라고 이름(alias) 붙였습니다.</span>
    <span class="n">relu</span> <span class="o">=</span> <span class="n">MyReLU</span><span class="o">.</span><span class="n">apply</span>

    <span class="c1"># 순전파 단계: Variable 연산을 사용하여 y 값을 예측합니다;</span>
    <span class="c1"># 사용자 정의 autograd 연산을 사용하여 ReLU를 계산합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w1</span><span class="p">))</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>

    <span class="c1"># 손실(loss)을 계산하고 출력합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># autograde를 사용하여 역전파 단계를 계산합니다.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 경사하강법(Gradient Descent)을 사용하여 가중치를 갱신합니다.</span>
    <span class="n">w1</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
    <span class="n">w2</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">w2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>

    <span class="c1"># 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.</span>
    <span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">w2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="tensorflow-static-graph">
<h3><a class="toc-backref" href="#id19">TensorFlow: 정적 그래프(Static Graph)</a><a class="headerlink" href="#tensorflow-static-graph" title="Permalink to this headline">¶</a></h3>
<p>PyTorch autograd는 Tensorflow와 많이 닮아보입니다: 두 프레임워크 모두 연산 그래프를
정의하며, 자동 미분을 사용하여 변화도를 계산합니다. 두 프레임워크의 가장 큰 차이점은
Tensorflow의 연산 그래프는 <strong>정적</strong> 이며, PyTorch는 <strong>동적</strong> 연산 그래프를 사용한다는
것입니다.</p>
<p>Tensorflow에서는 연산 그래프를 한 번 정의한 후 동일한 그래프를 계속해서 실행하며
가능한 다른 입력 데이터를 전달합니다. PyTorch에서는 각각의 순전파 단계에서 새로운
연산 그래프를 정의합니다.</p>
<p>정적 그래프는 먼저(Up-front) 그래프를 최적화할 수 있기 때문에 좋습니다; 예를 들어
프레임워크가 효율을 위해 일부 그래프 연산을 합치거나, 여러 GPU나 시스템(machine)에
그래프를 배포하는 전략을 제시할 수 있습니다. 만약 동일한 그래프를 계속 재사용하면,
같은 그래프가 반복되면서 비싼(Costly) 최적화 비용을 잠재적으로 상환할 수 있습니다.</p>
<p>정적 그래프와 동적 그래프는 제어 흐름(Control flow) 측면에서도 다릅니다. 어떤
모델에서 각 데이터 지점(Point)마다 다른 연산 연산을 수행하고 싶을 수 있습니다;
예를 들어 순환 신경망에서 각각의 데이터 지점마다 서로 다른 횟수만큼 펼칠(Unroll)
수 있습니다; 이러한 펼침은 반복문(Loop)으로 구현할 수 있습니다. 정적 그래프에서
반복문은 그래프의 일부가 돼야 합니다; 이러한 이유에서 Tensorflow는 그래프 내에
반복문을 포함하기 위해 <code class="docutils literal notranslate"><span class="pre">tf.scan</span></code> 과 같은 연산자를 제공합니다. 동적 그래프에서는
이러한 상황이 더 단순(Simple)해집니다: 각 예제에 대한 그래프를 즉석(on-the-fly)에서
작성하기 때문에, 일반적인 명령형(Imperative) 제어 흐름을 사용하여 각각의 입력에 따라
다른 계산을 수행할 수 있습니다.</p>
<p>위의 PyTorch autograd 예제와는 대조적으로, TensorFlow를 사용하여 간단한 2-계층
신경망을 구성하겠습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 먼저 연산 그래프를 구성하겠습니다:</span>

<span class="c1"># N은 배치 크기이며, D_in은 입력의 차원입니다;</span>
<span class="c1"># H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># 입력과 정답(target) 데이터를 위한 Placeholder를 생성합니다; 이는 우리가 그래프를</span>
<span class="c1"># 실행할 때 실제 데이터로 채워질 것입니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">D_in</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">D_out</span><span class="p">))</span>

<span class="c1"># 가중치를 저장하기 위한 Variable을 생성하고 무작위 데이터로 초기화합니다.</span>
<span class="c1"># Tensorflow의 Variable은 그래프가 실행되는 동안 그 값이 유지됩니다.</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">((</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)))</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">((</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)))</span>

<span class="c1"># 순전파 단계: Tensorflow의 Tensor 연산을 사용하여 y 값을 예측합니다.</span>
<span class="c1"># 이 코드가 어떠한 수치 연산을 실제로 수행하지는 않는다는 것을 유의하세요;</span>
<span class="c1"># 이 단계에서는 나중에 실행할 연산 그래프를 구성하기만 합니다.</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span>
<span class="n">h_relu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_relu</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>

<span class="c1"># Tensorflow의 Tensor 연산을 사용하여 손실(loss)을 계산합니다.</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># 손실에 따른 w1, w2의 변화도(Gradient)를 계산합니다.</span>
<span class="n">grad_w1</span><span class="p">,</span> <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">])</span>

<span class="c1"># 경사하강법(Gradient Descent)을 사용하여 가중치를 갱신합니다. 실제로 가중치를</span>
<span class="c1"># 갱신하기 위해서는 그래프가 실행될 때 new_w1과 new_w2 계산(evaluate)해야 합니다.</span>
<span class="c1"># Tensorflow에서 가중치의 값을 갱신하는 작업은 연산 그래프의 일부임을 유의하십시오;</span>
<span class="c1"># PyTorch에서는 이 작업이 연산 그래프의 밖에서 일어납니다.</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">new_w1</span> <span class="o">=</span> <span class="n">w1</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">w1</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1</span><span class="p">)</span>
<span class="n">new_w2</span> <span class="o">=</span> <span class="n">w2</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">w2</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2</span><span class="p">)</span>

<span class="c1"># 지금까지 우리는 연산 그래프를 구성하였으므로, 실제로 그래프를 실행하기 위해 이제</span>
<span class="c1"># Tensorflow 세션(Session)에 들어가보겠습니다.</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c1"># 그래프를 한 번 실행하여 Variable w1과 w2를 초기화합니다.</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

    <span class="c1"># 입력 데이터 x와 정답 데이터 y를 저장하기 위한 NumPy 배열을 생성합니다.</span>
    <span class="n">x_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">)</span>
    <span class="n">y_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
        <span class="c1"># 그래프를 여러 번 실행합니다. 매번 그래프가 실행할 때마다 feed_dict</span>
        <span class="c1"># 인자(argument)로 명시하여 x_value를 x에, y_value를 y에 할당(bind)하고자</span>
        <span class="c1"># 합니다. 또한, 그래프를 실행할 때마다 손실과 new_w1, new_w2 값을</span>
        <span class="c1"># 계산하려고 합니다; 이러한 Tensor들의 값은 NumPy 배열로 반환됩니다.</span>
        <span class="n">loss_value</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">new_w1</span><span class="p">,</span> <span class="n">new_w2</span><span class="p">],</span>
                                    <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">x_value</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_value</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">loss_value</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="nn">
<h2><a class="toc-backref" href="#id20"><cite>nn</cite> 모듈</a><a class="headerlink" href="#nn" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pytorch-nn">
<h3><a class="toc-backref" href="#id21">PyTorch: nn</a><a class="headerlink" href="#pytorch-nn" title="Permalink to this headline">¶</a></h3>
<p>연산 그래프와 autograd는 복잡한 연산자를 정의하고 도함수(derivative)를 자동으로
계산하는데 매우 강력한 패러다임(paradigm)입니다; 하지만 규모가 큰 신경망에서는
autograd 그 자체만으로는 너무 낮은 수준(low-level)일 수 있습니다.</p>
<p>신경망을 구성할 때 종종 연산을 여러 <strong>계층</strong> 으로 배열하는 것으로 생각하게 되는데,
이 중 일부는 학습 도중 최적화가 될 <strong>학습 가능한 매개변수</strong> 를 갖고 있습니다.</p>
<p>Tensorflow에서 <a class="reference external" href="https://github.com/fchollet/keras">Keras</a>,
<a class="reference external" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">TensorFlow-Slim</a>,
나 <a class="reference external" href="http://tflearn.org/">TFLearn</a> 같은 패키지는 원초적(raw)인 연산 그래프보다
더 높은 수준의 추상화(higher-level abstraction)를 제공하여 신경망을 구축하는데
있어 유용합니다.</p>
<p>PyTorch에서는 <code class="docutils literal notranslate"><span class="pre">nn</span></code> 패키지가 동일한 목적으로 제공됩니다. <code class="docutils literal notranslate"><span class="pre">nn</span></code> 패키지는
대략 신경망 계층들과 동일한 <strong>모듈</strong> 의 집합을 정의합니다.
모듈은 입력 Variable을 받고 출력 Variable을 계산하는 한편, 학습 가능한
매개변수를 포함하는 Variable과 같은 내부 상태(internal state)를 갖습니다.
또한, <code class="docutils literal notranslate"><span class="pre">nn</span></code> 패키지는 신경망을 학습시킬 때 주로 사용하는 유용한 손실 함수들도
정의합니다.</p>
<p>이번 예제에서는 <code class="docutils literal notranslate"><span class="pre">nn</span></code> 패키지를 이용하여 2-계층 신경망을 구성해보겠습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>

<span class="c1"># N은 배치 크기이며, D_in은 입력의 차원입니다;</span>
<span class="c1"># H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로</span>
<span class="c1"># 감쌉니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># nn 패키지를 사용하여 모델을 순차적인 계층(Sequence of layers)으로 정의합니다.</span>
<span class="c1"># nn.Sequential은 다른 모듈들을 포함하는 모듈로, 그 모듈들을 순차적으로 적용하여</span>
<span class="c1"># 출력을 생성합니다. 각각의 선형(Linear) 모듈은 선형 함수를 사용하여 입력으로부터</span>
<span class="c1"># 출력을 계산하고, 가중치와 편향(Bias)을 저장하기 위해 내부적인 Variable을 갖습니다.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># 또한, nn 패키지에는 널리 사용하는 손실 함수들에 대한 정의도 포함하고 있습니다;</span>
<span class="c1"># 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용하겠습니다.</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 모델에 x를 전달하여 예상하는 y 값을 계산합니다. 모듈 객체는</span>
    <span class="c1"># __call__ 연산자를 덮어써서(Override) 함수처럼 호출할 수 있게 합니다.</span>
    <span class="c1"># 그렇게 함으로써 입력 데이터의 Variable을 모듈에 전달하고 출력 데이터의</span>
    <span class="c1"># Variable을 생성합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># 손실을 계산하고 출력합니다. 예측한 y값과 정답 y를 갖는 Variable들을 전달하고,</span>
    <span class="c1"># 손실 함수는 손실(loss)을 갖는 Variable을 반환합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># 역전파 단계를 실행하기 전에 변화도를 0으로 만듭니다.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해서 손실의 변화도를</span>
    <span class="c1"># 계산합니다. 내부적으로 각 모듈의 매개변수는 requires_grad=True 일 때</span>
    <span class="c1"># Variable 내에 저장되므로, 이 호출은 모든 모델의 모든 학습 가능한 매개변수의</span>
    <span class="c1"># 변화도를 계산하게 됩니다.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 경사하강법(Gradient Descent)를 사용하여 가중치를 갱신합니다. 각 매개변수는</span>
    <span class="c1"># Variable이므로 이전에 했던 것과 같이 데이터와 변화도에 접근할 수 있습니다.</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
</div>
<div class="section" id="pytorch-optim">
<h3><a class="toc-backref" href="#id22">PyTorch: optim</a><a class="headerlink" href="#pytorch-optim" title="Permalink to this headline">¶</a></h3>
<p>Up to this point we have updated the weights of our models by manually
mutating the <code class="docutils literal notranslate"><span class="pre">.data</span></code> member for Variables holding learnable
parameters. This is not a huge burden for simple optimization algorithms
like stochastic gradient descent, but in practice we often train neural
networks using more sophisticated optimizers like AdaGrad, RMSProp,
Adam, etc.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">optim</span></code> package in PyTorch abstracts the idea of an optimization
algorithm and provides implementations of commonly used optimization
algorithms.</p>
<p>In this example we will use the <code class="docutils literal notranslate"><span class="pre">nn</span></code> package to define our model as
before, but we will optimize the model using the Adam algorithm provided
by the <code class="docutils literal notranslate"><span class="pre">optim</span></code> package:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>

<span class="c1"># N은 배치 크기이며, D_in은 입력의 차원입니다;</span>
<span class="c1"># H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로</span>
<span class="c1"># 감쌉니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># nn 패키지를 사용하여 모델과 손실 함수를 정의합니다.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># optim 패키지를 사용하여 모델의 가중치를 갱신할 Optimizer를 정의합니다.</span>
<span class="c1"># 여기서는 Adam을 사용하겠습니다; optim 패키지는 다른 다양한 최적화 알고리즘을</span>
<span class="c1"># 포함하고 있습니다. Adam 생성자의 첫번째 인자는 갱신해야 하는 Variable을</span>
<span class="c1"># Optimizer에 알려줍니다.</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 모델에 x를 전달하여 예상하는 y 값을 계산합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># 손실을 계산하고 출력합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># 역전파 단계 전에, Optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인)</span>
    <span class="c1"># 갱신할 Variable들에 대한 모든 변화도를 0으로 만듭니다. 이는 기본적으로,</span>
    <span class="c1"># .backward()를 호출할 때마다 변화도가 버퍼(Buffer)에 (덮어쓰지 않고) 누적되기</span>
    <span class="c1"># 때문입니다. 더 자세한 내용은 torch.autograd.backward에 대한 문서를 참조하세요.</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 역전파 단계: 모델의 매개변수에 대한 손실의 변화도를 계산합니다.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="pytorch-custom-nn-modules">
<h3><a class="toc-backref" href="#id23">PyTorch: Custom nn Modules</a><a class="headerlink" href="#pytorch-custom-nn-modules" title="Permalink to this headline">¶</a></h3>
<p>Sometimes you will want to specify models that are more complex than a
sequence of existing Modules; for these cases you can define your own
Modules by subclassing <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and defining a <code class="docutils literal notranslate"><span class="pre">forward</span></code> which
receives input Variables and produces output Variables using other
modules or other autograd operations on Variables.</p>
<p>In this example we implement our two-layer network as a custom Module
subclass:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>


<span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        생성자에서 2개의 nn.Linear 모듈을 생성(Instantiate)하고, 멤버 변수로</span>
<span class="sd">        지정합니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TwoLayerNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        순전파 함수에서는 입력 데이터의 Variable을 받아서 출력 데이터의 Variable을</span>
<span class="sd">        반환해야 합니다. Variable 상의 임의의 연산자뿐만 아니라 생성자에서 정의한</span>
<span class="sd">        모듈을 사용할 수 있습니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">h_relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">h_relu</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span>


<span class="c1"># N은 배치 크기이며, D_in은 입력의 차원입니다;</span>
<span class="c1"># H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로</span>
<span class="c1"># 감쌉니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># 앞에서 정의한 클래스를 생성(Instantiating)해서 모델을 구성합니다.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

<span class="c1"># 손실함수와 Optimizer를 만듭니다. SGD 생성자에서 model.parameters()를 호출하면</span>
<span class="c1"># 모델의 멤버인 2개의 nnLinear 모듈의 학습 가능한 매개변수들이 포함됩니다.</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 모델에 x를 전달하여 예상하는 y 값을 계산합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># 손실을 계산하고 출력합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="pytorch-control-flow-weight-sharing">
<h3><a class="toc-backref" href="#id24">PyTorch: Control Flow + Weight Sharing</a><a class="headerlink" href="#pytorch-control-flow-weight-sharing" title="Permalink to this headline">¶</a></h3>
<p>As an example of dynamic graphs and weight sharing, we implement a very
strange model: a fully-connected ReLU network that on each forward pass
chooses a random number between 1 and 4 and uses that many hidden
layers, reusing the same weights multiple times to compute the innermost
hidden layers.</p>
<p>For this model we can use normal Python flow control to implement the loop,
and we can implement weight sharing among the innermost layers by simply
reusing the same Module multiple times when defining the forward pass.</p>
<p>We can easily implement this model as a Module subclass:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>


<span class="k">class</span> <span class="nc">DynamicNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        생성자에서는 순전파 단계에서 사용할 3개의 nn.Linear 개체(Instance)를 생성합니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DynamicNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">middle_linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        모델의 순전파 단계에서, 무작위로 0, 1, 2 또는 3 중에 하나를 선택하고</span>
<span class="sd">        은닉 계층 표현(representation)을 계산하기 위해 여러번 사용한 middle_linear</span>
<span class="sd">        모듈을 재사용합니다.</span>

<span class="sd">        각 순전파 단계에서 동적 연산 그래프를 구성하기 때문에, 모델의 순전파 단계를</span>
<span class="sd">        정의할 때 반복문이나 조건문과 같이 일반적인 Python 제어 흐름 연산자를 사용할</span>
<span class="sd">        수 있습니다.</span>

<span class="sd">        여기에서 연산 그래프를 정의할 때 동일한 모듈을 여러번 재사용하는 것이</span>
<span class="sd">        완벽하게 안전하다는 것을 알 수 있습니다. 이것이 각 모듈을 한 번만 사용하는</span>
<span class="sd">        Lua Torch보다 크게 개선된 부분입니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">h_relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)):</span>
            <span class="n">h_relu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">middle_linear</span><span class="p">(</span><span class="n">h_relu</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_linear</span><span class="p">(</span><span class="n">h_relu</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span>


<span class="c1"># N은 배치 크기이며, D_in은 입력의 차원입니다;</span>
<span class="c1"># H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로</span>
<span class="c1"># 감쌉니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># 앞에서 정의한 클래스를 생성(Instantiating)해서 모델을 구성합니다.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DynamicNet</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

<span class="c1"># 손실함수와 Optimizer를 만듭니다. 이 이상한 모델을 순수한 확률적 경사 하강법</span>
<span class="c1"># (Stochastic Gradient Decent)으로 학습하는 것은 어려우므로, momentum을 사용합니다.</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 모델에 x를 전달하여 예상하는 y 값을 계산합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># 손실을 계산하고 출력합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="examples">
<span id="examples-download"></span><h2><a class="toc-backref" href="#id25">Examples</a><a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<p>You can browse the above examples here.</p>
<div class="section" id="tensors">
<h3><a class="toc-backref" href="#id26">Tensors</a><a class="headerlink" href="#tensors" title="Permalink to this headline">¶</a></h3>
<div class="toctree-wrapper compound">
</div>
<p><div class="sphx-glr-thumbcontainer" tooltip="하나의 은닉 계층과 편향(Bias)이 없는 완전히 연결된 ReLU 신경망에 유클리드 오차(Euclidean Error)를 사용하여 x로부터 y를 예측하도록 학습하겠습니다."><div class="figure" id="id4">
<img alt="../_images/sphx_glr_two_layer_net_numpy_thumb.png" src="../_images/sphx_glr_two_layer_net_numpy_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="examples_tensor/two_layer_net_numpy.html#sphx-glr-beginner-examples-tensor-two-layer-net-numpy-py"><span class="std std-ref">준비 운동: NumPy</span></a></span></p>
</div>
</div></p>
<p><div class="sphx-glr-thumbcontainer" tooltip="하나의 은닉 계층과 편향(Bias)이 없는 완전히 연결된 ReLU 신경망에 유클리드 거리(Euclidean Distance)의 제곱을 최소화하여 x로부터 y를 예측하도록 ..."><div class="figure" id="id5">
<img alt="../_images/sphx_glr_two_layer_net_tensor_thumb.png" src="../_images/sphx_glr_two_layer_net_tensor_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="examples_tensor/two_layer_net_tensor.html#sphx-glr-beginner-examples-tensor-two-layer-net-tensor-py"><span class="std std-ref">PyTorch: Tensor</span></a></span></p>
</div>
</div></p>
<div style='clear:both'></div></div>
<div class="section" id="id3">
<h3><a class="toc-backref" href="#id27">Autograd</a><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<div class="toctree-wrapper compound">
</div>
<p><div class="sphx-glr-thumbcontainer" tooltip="하나의 은닉 계층(Hidden Layer)과 편향(Bias)이 없는 완전히 연결된 ReLU 신경망에 유클리드 거리(Euclidean Distance)의 제곱을 최소화하여 ..."><div class="figure" id="id6">
<img alt="../_images/sphx_glr_two_layer_net_autograd_thumb.png" src="../_images/sphx_glr_two_layer_net_autograd_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="examples_autograd/two_layer_net_autograd.html#sphx-glr-beginner-examples-autograd-two-layer-net-autograd-py"><span class="std std-ref">PyTorch: Variable과 autograd</span></a></span></p>
</div>
</div></p>
<p><div class="sphx-glr-thumbcontainer" tooltip="하나의 은닉 계층(Hidden Layer)과 편향(Bias)이 없는 완전히 연결된 ReLU 신경망에 유클리드 거리(Euclidean Distance)의 제곱을 최소화하여 ..."><div class="figure" id="id7">
<img alt="../_images/sphx_glr_two_layer_net_custom_function_thumb.png" src="../_images/sphx_glr_two_layer_net_custom_function_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="examples_autograd/two_layer_net_custom_function.html#sphx-glr-beginner-examples-autograd-two-layer-net-custom-function-py"><span class="std std-ref">PyTorch: 새 autograd 함수 정의하기</span></a></span></p>
</div>
</div></p>
<p><div class="sphx-glr-thumbcontainer" tooltip="하나의 은닉 계층(Hidden Layer)과 편향(Bias)이 없는 완전히 연결된 ReLU 신경망에 유클리드 거리(Euclidean Distance)의 제곱을 최소화하여 ..."><div class="figure" id="id8">
<img alt="../_images/sphx_glr_tf_two_layer_net_thumb.png" src="../_images/sphx_glr_tf_two_layer_net_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="examples_autograd/tf_two_layer_net.html#sphx-glr-beginner-examples-autograd-tf-two-layer-net-py"><span class="std std-ref">TensorFlow: 정적 그래프(Static Graph)</span></a></span></p>
</div>
</div></p>
<div style='clear:both'></div></div>
<div class="section" id="nn-module">
<h3><a class="toc-backref" href="#id28"><cite>nn</cite> module</a><a class="headerlink" href="#nn-module" title="Permalink to this headline">¶</a></h3>
<div class="toctree-wrapper compound">
</div>
<p><div class="sphx-glr-thumbcontainer" tooltip="하나의 은닉 계층(Hidden Layer)을 갖는 완전히 연결된 ReLU 신경망에 유클리드 거리(Euclidean Distance)의 제곱을 최소화하여 x로부터 y를 예측..."><div class="figure" id="id9">
<img alt="../_images/sphx_glr_two_layer_net_nn_thumb.png" src="../_images/sphx_glr_two_layer_net_nn_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="examples_nn/two_layer_net_nn.html#sphx-glr-beginner-examples-nn-two-layer-net-nn-py"><span class="std std-ref">PyTorch: nn</span></a></span></p>
</div>
</div></p>
<p><div class="sphx-glr-thumbcontainer" tooltip="하나의 은닉 계층(Hidden Layer)을 갖는 완전히 연결된 ReLU 신경망에 유클리드 거리(Euclidean Distance)의 제곱을 최소화하여 x로부터 y를 예측..."><div class="figure" id="id10">
<img alt="../_images/sphx_glr_two_layer_net_optim_thumb.png" src="../_images/sphx_glr_two_layer_net_optim_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="examples_nn/two_layer_net_optim.html#sphx-glr-beginner-examples-nn-two-layer-net-optim-py"><span class="std std-ref">PyTorch: optim</span></a></span></p>
</div>
</div></p>
<p><div class="sphx-glr-thumbcontainer" tooltip="하나의 은닉 계층(Hidden Layer)을 갖는 완전히 연결된 ReLU 신경망에 유클리드 거리(Euclidean Distance)의 제곱을 최소화하여 x로부터 y를 예측..."><div class="figure" id="id11">
<img alt="../_images/sphx_glr_two_layer_net_module_thumb.png" src="../_images/sphx_glr_two_layer_net_module_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="examples_nn/two_layer_net_module.html#sphx-glr-beginner-examples-nn-two-layer-net-module-py"><span class="std std-ref">PyTorch: 사용자 정의 nn 모듈</span></a></span></p>
</div>
</div></p>
<p><div class="sphx-glr-thumbcontainer" tooltip="PyTorch 동적 그래프의 성능을 보여주기 위해, 매우 이상한 모델을 구현해보겠습니다: 각각의 순전파 단계에서 많은 은닉 계층을 갖는 완전히 연결(Fully-connec..."><div class="figure" id="id12">
<img alt="../_images/sphx_glr_dynamic_net_thumb.png" src="../_images/sphx_glr_dynamic_net_thumb.png" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="examples_nn/dynamic_net.html#sphx-glr-beginner-examples-nn-dynamic-net-py"><span class="std std-ref">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</span></a></span></p>
</div>
</div></p>
<div style='clear:both'></div></div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="examples_tensor/two_layer_net_numpy.html" class="btn btn-neutral float-right" title="준비 운동: NumPy" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="former_torchies/parallelism_tutorial.html" class="btn btn-neutral" title="Multi-GPU examples" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, PyTorch.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.3.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-71919972-2', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>