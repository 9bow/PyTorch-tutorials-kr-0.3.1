

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Pytorch로 분산 어플리케이션 개발하기 &mdash; PyTorch Tutorials 0.3.1 documentation</title>

















    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />



    <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />

    <link rel="stylesheet" href="../_static/css/pytorch_theme.css" type="text/css" />

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />



        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="PyTorch Tutorials 0.3.1 documentation" href="../index.html"/>
        <link rel="next" title="Spatial Transformer Networks Tutorial" href="spatial_transformer_tutorial.html"/>
        <link rel="prev" title="Reinforcement Learning (DQN) tutorial" href="reinforcement_q_learning.html"/>


  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">
  <a href="https://tutorials.pytorch.kr/" class="latest-version-tease">최신 버전의 PyTorch 튜토리얼을 만나보세요!</a>



  <div class="wy-grid-for-nav">


    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">



            <a href="../index.html" class="icon icon-home"> PyTorch Tutorials




            <img src="../_static/pytorch-logo-dark.svg" class="logo" />

          </a>




              <div class="version">
                0.3.1
              </div>




<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">






              <p class="caption"><span class="caption-text">Beginner Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_60min_blitz.html">PyTorch로 딥러닝하기: 60분만에 끝장내기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html">PyTorch가 무엇인가요?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#id1">시작하기</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#tensors">Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#operations">연산(Operations)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#numpy-bridge">NumPy 변환(Bridge)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#torch-tensor-numpy">Torch Tensor를 NumPy 배열로 변환하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#numpy-torch-tensor">NumPy 배열을 Torch Tensor로 변환하기</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#cuda-tensors">CUDA Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/blitz/autograd_tutorial.html">Autograd: 자동 미분</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/autograd_tutorial.html#variable">변수(Variable)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/autograd_tutorial.html#gradient">변화도(Gradient)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/blitz/neural_networks_tutorial.html">신경망(Neural Networks)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/neural_networks_tutorial.html#id1">신경망 정의하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/neural_networks_tutorial.html#loss-function">손실 함수 (Loss Function)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/neural_networks_tutorial.html#backprop">역전파(Backprop)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/neural_networks_tutorial.html#id4">가중치 갱신</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html">분류기(Classifier) 학습하기</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#id1">데이터는 어떻게 하나요?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#id2">이미지 분류기 학습하기</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#cifar10">1. CIFAR10를 불러오고 정규화하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#convolution-neural-network">2. 합성곱 신경망(Convolution Neural Network) 정의하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#optimizer">3. 손실 함수와 Optimizer 정의하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#id3">4. 신경망 학습하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#id4">5. 시험용 데이터로 신경망 검사하기</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#gpu">GPU에서 학습하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#id5">여러개의 GPU에서 학습하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#id6">이제 뭘 해볼까요?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html">Optional: Data Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#imports-and-parameters">Imports and parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#dummy-dataset">Dummy DataSet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#simple-model">Simple Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#create-model-and-dataparallel">Create Model and DataParallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#run-the-model">Run the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#results">Results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#gpus">2 GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#id1">3 GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#id2">8 GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/former_torchies_tutorial.html">Torch 사용자를 위한 PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html">Tensor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#in-place-out-of-place">In-place / Out-of-place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#id1">0-인덱스</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#camel-case">카멜표기법(Camel Case) 없음</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#numpy-bridge">NumPy 변환(Bridge)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#torch-tensor-numpy">Torch Tensor를 NumPy 배열로 변환하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#numpy-torch-tensor">NumPy 배열을 Torch Tensor로 변환하기</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#cuda-tensors">CUDA Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/former_torchies/autograd_tutorial.html">Autograd</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/autograd_tutorial.html#variable">변수(Variable)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/autograd_tutorial.html#gradient">변화도(Gradient)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/former_torchies/nn_tutorial.html">nn 패키지</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/nn_tutorial.html#convnet">예제1: 합성곱 신경망(ConvNet)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/nn_tutorial.html#hook">순방향/역방향 함수 훅(Hook)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/nn_tutorial.html#recurrent-nets">예제2: 순환 신경망(Recurrent Nets)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/former_torchies/parallelism_tutorial.html">멀티-GPU 예제</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/parallelism_tutorial.html#dataparallel">DataParallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/parallelism_tutorial.html#cpu-gpu">모델의 일부는 CPU, 일부는 GPU에서</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/pytorch_with_examples.html">예제로 배우는 PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/pytorch_with_examples.html#tensor">Tensor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#numpy">준비 운동: NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-tensor">PyTorch: Tensor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/pytorch_with_examples.html#autograd">Autograd</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-variables-autograd">PyTorch: Variables과 autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-autograd">PyTorch: 새 autograd 함수 정의하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#tensorflow-static-graph">TensorFlow: 정적 그래프(Static Graph)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/pytorch_with_examples.html#nn"><cite>nn</cite> 모듈</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-nn">PyTorch: nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-optim">PyTorch: optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#id3">PyTorch: 사용자 정의 nn 모듈</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-control-flow-weight-sharing">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/pytorch_with_examples.html#examples-download">예제 코드</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#id5">Tensor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_tensor/two_layer_net_numpy.html">준비 운동: NumPy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_tensor/two_layer_net_tensor.html">PyTorch: Tensor</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#id6">Autograd</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_autograd/two_layer_net_autograd.html">PyTorch: Variable과 autograd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_autograd/two_layer_net_custom_function.html">PyTorch: 새 autograd 함수 정의하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_autograd/tf_two_layer_net.html">TensorFlow: 정적 그래프(Static Graph)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#id7"><cite>nn</cite> 모듈</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_nn/two_layer_net_nn.html">PyTorch: nn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_nn/two_layer_net_optim.html">PyTorch: optim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_nn/two_layer_net_module.html">PyTorch: 사용자 정의 nn 모듈</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_nn/dynamic_net.html">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">전이학습(Transfer Learning) 튜토리얼</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#id2">데이터 불러오기</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#id4">일부 이미지 시각화하기</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#id5">모델 학습하기</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#id6">모델 예측값 시각화하기</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#finetuning">합성곱 신경망 미세조정(Finetuning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#id7">학습 및 평가하기</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#id8">고정 특정 추출기로써의 합성곱 신경망</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#id9">학습 및 평가하기</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/data_loading_tutorial.html">Data Loading and Processing Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/data_loading_tutorial.html#dataset-class">Dataset class</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/data_loading_tutorial.html#transforms">Transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/data_loading_tutorial.html#compose-transforms">Compose transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/data_loading_tutorial.html#iterating-through-the-dataset">Iterating through the dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/data_loading_tutorial.html#afterword-torchvision">Afterword: torchvision</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_nlp_tutorial.html">Deep Learning for NLP with Pytorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html">Introduction to PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html#introduction-to-torch-s-tensor-library">Introduction to Torch’s tensor library</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html#creating-tensors">Creating Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html#operations-with-tensors">Operations with Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html#reshaping-tensors">Reshaping Tensors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html#computation-graphs-and-automatic-differentiation">Computation Graphs and Automatic Differentiation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html">Deep Learning with PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#deep-learning-building-blocks-affine-maps-non-linearities-and-objectives">Deep Learning Building Blocks: Affine maps, non-linearities and objectives</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#affine-maps">Affine Maps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#non-linearities">Non-Linearities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#softmax-and-probabilities">Softmax and Probabilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#objective-functions">Objective Functions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#optimization-and-training">Optimization and Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#creating-network-components-in-pytorch">Creating Network Components in Pytorch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#example-logistic-regression-bag-of-words-classifier">Example: Logistic Regression Bag-of-Words classifier</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/nlp/word_embeddings_tutorial.html">Word Embeddings: Encoding Lexical Semantics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/word_embeddings_tutorial.html#getting-dense-word-embeddings">Getting Dense Word Embeddings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/word_embeddings_tutorial.html#word-embeddings-in-pytorch">Word Embeddings in Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/word_embeddings_tutorial.html#an-example-n-gram-language-modeling">An Example: N-Gram Language Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words">Exercise: Computing Word Embeddings: Continuous Bag-of-Words</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/nlp/sequence_models_tutorial.html">Sequence Models and Long-Short Term Memory Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/sequence_models_tutorial.html#lstm-s-in-pytorch">LSTM’s in Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging">Example: An LSTM for Part-of-Speech Tagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/sequence_models_tutorial.html#exercise-augmenting-the-lstm-part-of-speech-tagger-with-character-level-features">Exercise: Augmenting the LSTM part-of-speech tagger with character-level features</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/nlp/advanced_tutorial.html">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/advanced_tutorial.html#dynamic-versus-static-deep-learning-toolkits">Dynamic versus Static Deep Learning Toolkits</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/advanced_tutorial.html#bi-lstm-conditional-random-field-discussion">Bi-LSTM Conditional Random Field Discussion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/advanced_tutorial.html#implementation-notes">Implementation Notes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/advanced_tutorial.html#exercise-a-new-loss-function-for-discriminative-tagging">Exercise: A new loss function for discriminative tagging</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Intermediate Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="char_rnn_classification_tutorial.html">문자 단위 RNN으로 이름 분류하기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="char_rnn_classification_tutorial.html#id2">데이터 준비하기</a><ul>
<li class="toctree-l3"><a class="reference internal" href="char_rnn_classification_tutorial.html#tensor">이름을 Tensor 로 변경</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="char_rnn_classification_tutorial.html#id3">네트워크 생성</a></li>
<li class="toctree-l2"><a class="reference internal" href="char_rnn_classification_tutorial.html#id4">학습</a><ul>
<li class="toctree-l3"><a class="reference internal" href="char_rnn_classification_tutorial.html#id5">학습 준비</a></li>
<li class="toctree-l3"><a class="reference internal" href="char_rnn_classification_tutorial.html#id6">네트워크 학습</a></li>
<li class="toctree-l3"><a class="reference internal" href="char_rnn_classification_tutorial.html#id7">결과 도식화</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="char_rnn_classification_tutorial.html#id8">결과 평가</a><ul>
<li class="toctree-l3"><a class="reference internal" href="char_rnn_classification_tutorial.html#id9">사용자 입력으로 실행</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="char_rnn_classification_tutorial.html#id10">연습</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_generation_tutorial.html">문자 단위 RNN으로 이름 생성하기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="char_rnn_generation_tutorial.html#id2">데이터 준비하기</a></li>
<li class="toctree-l2"><a class="reference internal" href="char_rnn_generation_tutorial.html#id4">네트워크 생성</a></li>
<li class="toctree-l2"><a class="reference internal" href="char_rnn_generation_tutorial.html#id5">학습</a><ul>
<li class="toctree-l3"><a class="reference internal" href="char_rnn_generation_tutorial.html#id6">학습 준비</a></li>
<li class="toctree-l3"><a class="reference internal" href="char_rnn_generation_tutorial.html#id7">네트워크 학습</a></li>
<li class="toctree-l3"><a class="reference internal" href="char_rnn_generation_tutorial.html#id8">손실 도식화</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="char_rnn_generation_tutorial.html#id9">네트워크 샘플링</a></li>
<li class="toctree-l2"><a class="reference internal" href="char_rnn_generation_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="seq2seq_translation_tutorial.html">Sequence to Sequence 네트워크와 Attention을 이용한 번역</a><ul>
<li class="toctree-l2"><a class="reference internal" href="seq2seq_translation_tutorial.html#id2">데이터 파일 로딩</a></li>
<li class="toctree-l2"><a class="reference internal" href="seq2seq_translation_tutorial.html#seq2seq">Seq2Seq 모델</a><ul>
<li class="toctree-l3"><a class="reference internal" href="seq2seq_translation_tutorial.html#id4">인코더</a></li>
<li class="toctree-l3"><a class="reference internal" href="seq2seq_translation_tutorial.html#id5">디코더</a><ul>
<li class="toctree-l4"><a class="reference internal" href="seq2seq_translation_tutorial.html#id6">간단한 디코더</a></li>
<li class="toctree-l4"><a class="reference internal" href="seq2seq_translation_tutorial.html#id7">어텐션 디코더</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="seq2seq_translation_tutorial.html#id8">학습</a><ul>
<li class="toctree-l3"><a class="reference internal" href="seq2seq_translation_tutorial.html#id9">학습 데이터 준비</a></li>
<li class="toctree-l3"><a class="reference internal" href="seq2seq_translation_tutorial.html#id10">모델 학습</a></li>
<li class="toctree-l3"><a class="reference internal" href="seq2seq_translation_tutorial.html#id11">결과 도식화</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="seq2seq_translation_tutorial.html#id12">평가</a></li>
<li class="toctree-l2"><a class="reference internal" href="seq2seq_translation_tutorial.html#id13">학습과 평가</a><ul>
<li class="toctree-l3"><a class="reference internal" href="seq2seq_translation_tutorial.html#id14">어텐션 시각화</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="seq2seq_translation_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_q_learning.html">Reinforcement Learning (DQN) tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="reinforcement_q_learning.html#replay-memory">Replay Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="reinforcement_q_learning.html#dqn-algorithm">DQN algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="reinforcement_q_learning.html#q-network">Q-network</a></li>
<li class="toctree-l3"><a class="reference internal" href="reinforcement_q_learning.html#input-extraction">Input extraction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="reinforcement_q_learning.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="reinforcement_q_learning.html#hyperparameters-and-utilities">Hyperparameters and utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="reinforcement_q_learning.html#training-loop">Training loop</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Pytorch로 분산 어플리케이션 개발하기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#point-to-point-communication">지점간 통신(Point-to-Point Communication)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#collective-communication">집단 통신 (Collective Communication)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distributed-training">분산 학습(Distributed Training)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#our-own-ring-allreduce">Our Own Ring-Allreduce</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-topics">Advanced Topics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">통신 백엔드</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">초기화 방법</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="spatial_transformer_tutorial.html#loading-the-data">Loading the data</a></li>
<li class="toctree-l2"><a class="reference internal" href="spatial_transformer_tutorial.html#depicting-spatial-transformer-networks">Depicting spatial transformer networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="spatial_transformer_tutorial.html#training-the-model">Training the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="spatial_transformer_tutorial.html#visualizing-the-stn-results">Visualizing the STN results</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Advanced Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/neural_style_tutorial.html">Neural Transfer with PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#introduction">Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#neural-what">Neural what?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#how-does-it-work">How does it work?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../advanced/neural_style_tutorial.html#ok-how-does-it-work">OK. How does it work?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#pytorch-implementation">PyTorch implementation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#packages">Packages</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#cuda">Cuda</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#load-images">Load images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#display-images">Display images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#content-loss">Content loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#style-loss">Style loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#load-the-neural-network">Load the neural network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#input-image">Input image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#gradient-descent">Gradient descent</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html">Creating extensions using numpy and scipy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html#parameter-less-example">Parameter-less example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html#parametrized-example">Parametrized example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/super_resolution_with_caffe2.html">Transfering a model from PyTorch to Caffe2 and Mobile using ONNX</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/super_resolution_with_caffe2.html#transfering-srresnet-using-onnx">Transfering SRResNet using ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/super_resolution_with_caffe2.html#running-the-model-on-mobile-devices">Running the model on mobile devices</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/c_extension.html">C 언어로 PyTorch 확장 기능(custom extension) 만들기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/c_extension.html#c">1단계. C 코드 준비하기</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/c_extension.html#python">2단계. Python에서 불러오기</a></li>
</ul>
</li>
</ul>



        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">


      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">

          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PyTorch Tutorials</a>

      </nav>



      <div class="wy-nav-content">
        <div class="rst-content">
















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">

      <li><a href="../index.html">Docs</a> &raquo;</li>

      <li>Pytorch로 분산 어플리케이션 개발하기</li>


      <li class="wy-breadcrumbs-aside">


            <a href="../_sources/intermediate/dist_tuto.rst.txt" rel="nofollow"> View page source</a>


      </li>

  </ul>


  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">

  <div class="section" id="pytorch">
<h1>Pytorch로 분산 어플리케이션 개발하기<a class="headerlink" href="#pytorch" title="Permalink to this headline">¶</a></h1>
<dl class="docutils">
<dt><strong>Author</strong>: <a class="reference external" href="http://seba1511.com">Séb Arnold</a></dt>
<dd><strong>번역</strong>: <a class="reference external" href="https://github.com/adonisues">황성수</a></dd>
</dl>
<p>이 짧은 튜토리얼에서 Pytorch의 분산 패키지를 둘러봅니다. 분산 설정 방법을 살펴보고,
다른 통신 전략을 사용하고, 몇몇 내부 패키지를 확인해 봅니다.</p>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<!--
* Processes & machines
* variables and init_process_group
--><p>Pytorch에 포함된 분산 패키지 (i.e., <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>)는 연구자와 개발자가
여러개의 프로세서와 머신 클러스터에서 계산을 쉽게 병렬화하게 해준다.
그렇게 하기 위해서, messaging passing semantics 가 각 프로세스가 다른 프로세스들과
데이터를 통신하도록 해준다. 다중 처리(<code class="docutils literal notranslate"><span class="pre">torch.multiprocessing</span></code>) 패키지와 달리
프로세스는 다른 통신 백엔드를 사용할 수 있으며 동일한 기계에서 실행되는 것으로
제한됩니다.</p>
<p>시작하려면 여러 프로세스를 동시에 실행할 수 있어야합니다. 컴퓨트 클러스터에
접속할 경우 local sysadmin 으로 점검하거나 또는 선호하는 coordination tool을
사용하십시오.
(e.g.,
<a class="reference external" href="https://linux.die.net/man/1/pdsh">pdsh</a> ,
<a class="reference external" href="http://cea-hpc.github.io/clustershell/">clustershell</a> 또는
<a class="reference external" href="https://slurm.schedmd.com/">others</a> ) 이 튜토리얼에서는 다음 템플릿을 사용하여
단일 기기를 사용하고 여러 프로세스를 포크합니다.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;run.py:&quot;&quot;&quot;</span>
<span class="c1">#!/usr/bin/env python</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">torch.multiprocessing</span> <span class="k">import</span> <span class="n">Process</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Distributed function to be implemented later. &quot;&quot;&quot;</span>
    <span class="k">pass</span>

<span class="k">def</span> <span class="nf">init_processes</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;tcp&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Initialize the distributed environment. &quot;&quot;&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;29500&#39;</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_processes</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">run</span><span class="p">))</span>
        <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">processes</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
<p>위 스크립트는 각각 분산 환경을 설정하는 두개의 프로세스를 생성하고,
프로세스 그룹(<code class="docutils literal notranslate"><span class="pre">dist.init_process_group</span></code>)을 초기화하고, 마지막으로 주어진
<code class="docutils literal notranslate"><span class="pre">run</span></code> 함수를 실행합니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">init_processes</span></code> 함수는 동일한 IP 주소와 포트를 사용해서 모든 프로세스가 마스터를
통해서 조직 되게 한다. 우리는 TCP 백헨드를 사용했지만 대신
<a class="reference external" href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a> 또는
<a class="reference external" href="http://github.com/facebookincubator/gloo">Gloo</a> 를 사용할 수 있습니다.
(c.f. <a class="reference external" href="#communication-backends">Section 5.1</a>) 이 튜토리얼의 마지막에 있는
<code class="docutils literal notranslate"><span class="pre">dist.init_process_group</span></code> 에서 일어나는 마법을 살펴봅니다. 그러나 기본적으로
프로세스는 자신의 위치를 공유하여 서로 통신할 수 있습니다.</p>
</div>
<div class="section" id="point-to-point-communication">
<h2>지점간 통신(Point-to-Point Communication)<a class="headerlink" href="#point-to-point-communication" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center" id="id4">
<a class="reference internal image-reference" href="../_images/send_recv.png"><img alt="Send and Recv" src="../_images/send_recv.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">전송과 수신</span></p>
</div>
<p>하나의 프로세스에서 다른 프로세스로 데이터를 전송하는 것을 지점간 통신이라고합니다.
이것은 <code class="docutils literal notranslate"><span class="pre">send</span></code> 와 <code class="docutils literal notranslate"><span class="pre">recv</span></code> 함수 또는 직접 대응부인 (<em>immediate</em> counter-parts)
<code class="docutils literal notranslate"><span class="pre">isend</span></code> 와 <code class="docutils literal notranslate"><span class="pre">irecv</span></code> 를 통해 이루어집니다.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Blocking point-to-point communication.&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Send the tensor to process 1</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Receive tensor from process 0</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>위의 예제에서 두 프로세스는 모두 값이 0인 Tensor 로 시작하고, 0번 프로세스는
Tensor를 증가시키고 프로세스 1로 보내서 양쪽 모두 1.0으로 끝납니다. 프로세스 1은
수신 할 데이터를 저장하기 위해 메모리를 할당해야합니다.</p>
<p>또한 <code class="docutils literal notranslate"><span class="pre">send</span></code> / <code class="docutils literal notranslate"><span class="pre">recv</span></code> 는 <strong>blocking</strong> 으로 동작합니다. : 통신이 완료될 때까지
두 프로세스 모두 멈춥니다. 반면에 Immediates ( <code class="docutils literal notranslate"><span class="pre">isend</span></code> 와 <code class="docutils literal notranslate"><span class="pre">irecv</span></code> )는
<strong>non-blocking</strong> 으로 동작 합니다; 스크립트는 실행을 계속하고 메서드는 <code class="docutils literal notranslate"><span class="pre">wait()</span></code>
를 선택할 수 있는 <code class="docutils literal notranslate"><span class="pre">DistributedRequest</span></code> 객체를 반환합니다.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Non-blocking point-to-point communication.&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">req</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Send the tensor to process 1</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank 0 started sending&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Receive tensor from process 0</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">irecv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank 1 started receiving&#39;</span><span class="p">)</span>
    <span class="n">req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>Immediates 를 사용할 때 보내고 받는 Tensor에 대한 사용법에 주의해야 합니다.
언제 데이터가 다른 프로세스와 통신 될지 알지 못하기 때문에, <code class="docutils literal notranslate"><span class="pre">req.wait</span> <span class="pre">()</span></code> 가
완료되기 전에 전송된 Tensor를 수정하거나 수신된 Tensor에 접근해서는 안됩니다.</p>
<p>다시 말하면,</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">dist.isend</span> <span class="pre">()</span></code> 다음에 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 에 쓰면 정의되지 않은 동작이 발생합니다.</li>
<li><code class="docutils literal notranslate"><span class="pre">dist.irecv</span> <span class="pre">()</span></code> 다음에 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를 읽으면 정의되지 않은 동작이 발생합니다.</li>
</ul>
<p>그러나 <code class="docutils literal notranslate"><span class="pre">req.wait</span> <span class="pre">()</span></code> 가 실행 된 후에 통신이 이루어진 것과, <code class="docutils literal notranslate"><span class="pre">tensor[0]</span></code> 에
저장된 값이 1.0이라는 것이 보장됩니다.</p>
<p>지점 간 통신은 프로세스 통신에 대한 세분화 된 제어를 원할 때 유용합니다. 그것들은
<a class="reference external" href="https://github.com/baidu-research/baidu-allreduce">Baidu’s DeepSpeech</a> 또는
<a class="reference external" href="https://research.fb.com/publications/imagenet1kin1h/">Facebook’s large-scale experiments</a>
(c.f. <a class="reference external" href="#our-own-ring-allreduce">Section 4.1</a>) 와 같은 고급 알고리즘을 구현하는데
사용됩니다.</p>
</div>
<div class="section" id="collective-communication">
<h2>집단 통신 (Collective Communication)<a class="headerlink" href="#collective-communication" title="Permalink to this headline">¶</a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><div class="first last figure align-center" id="id5">
<a class="reference internal image-reference" href="../_images/scatter.png"><img alt="Scatter" src="../_images/scatter.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">Scatter</span></p>
</div>
</td>
<td><div class="first last figure align-center" id="id6">
<a class="reference internal image-reference" href="../_images/gather.png"><img alt="Gather" src="../_images/gather.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">Gather</span></p>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="first last figure align-center" id="id7">
<a class="reference internal image-reference" href="../_images/reduce.png"><img alt="Reduce" src="../_images/reduce.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">Reduce</span></p>
</div>
</td>
<td><div class="first last figure align-center" id="id8">
<a class="reference internal image-reference" href="../_images/all_reduce.png"><img alt="All-Reduce" src="../_images/all_reduce.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">All-Reduce</span></p>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="first last figure align-center" id="id9">
<a class="reference internal image-reference" href="../_images/broadcast.png"><img alt="Broadcast" src="../_images/broadcast.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">Broadcast</span></p>
</div>
</td>
<td><div class="first last figure align-center" id="id10">
<a class="reference internal image-reference" href="../_images/all_gather.png"><img alt="All-Gather" src="../_images/all_gather.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">All-Gather</span></p>
</div>
</td>
</tr>
</tbody>
</table>
<p>지점간 통신과는 달리 집단 통신은 <strong>그룹(Group)</strong> 의 모든 프로세스에서 통신 패턴을
허용합니다. 그룹은 모든 프로세스의 하위 집합입니다.
그룹을 만들려면, <code class="docutils literal notranslate"><span class="pre">dist.new_group</span> <span class="pre">(group)</span></code> 에 순위 목록을 전달하면 됩니다.
기본적으로 집단 통신은 <strong>월드(World)</strong> 라고도하는 모든 프로세스에서 실행됩니다.
예를 들어, 모든 프로세스에서 모든 Tensor의 합을 얻으려면,
<code class="docutils literal notranslate"><span class="pre">dist.all_reduce</span> <span class="pre">(tensor,</span> <span class="pre">op,</span> <span class="pre">group)</span></code> 를 사용할 수 있습니다.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; All-Reduce example.&quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Simple point-to-point communication. &quot;&quot;&quot;</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>그룹의 모든 Tensor의 합이 필요하기 때문에 Reduce 연산자로 <code class="docutils literal notranslate"><span class="pre">dist.reduce_op.SUM</span></code> 을
사용합니다. 일반적으로 교환 법칙이 성립하는 수학 연산은 연산자로 사용할 수 있습니다.</p>
<p>특별히, PyTorch는 4개의 연산자를 제공하고 모두 요소 별로(element-wise) 작동합니다.:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">dist.reduce_op.SUM</span></code>,</li>
<li><code class="docutils literal notranslate"><span class="pre">dist.reduce_op.PRODUCT</span></code>,</li>
<li><code class="docutils literal notranslate"><span class="pre">dist.reduce_op.MAX</span></code>,</li>
<li><code class="docutils literal notranslate"><span class="pre">dist.reduce_op.MIN</span></code>.</li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">dist.all_reduce</span> <span class="pre">(tensor,</span> <span class="pre">op,</span> <span class="pre">group)</span></code> 외에 현재 PyTorch에서 구현된 총 6개의
집단 통신이 있습니다.</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">dist.broadcast(tensor,</span> <span class="pre">src,</span> <span class="pre">group)</span></code>: <code class="docutils literal notranslate"><span class="pre">src</span></code> 에서 다른 모든 프로세스로
<code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를 복사합니다.</li>
<li><code class="docutils literal notranslate"><span class="pre">dist.reduce(tensor,</span> <span class="pre">dst,</span> <span class="pre">op,</span> <span class="pre">group)</span></code>: 모든 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 에 <code class="docutils literal notranslate"><span class="pre">op</span></code> 를 적용하고
그 결과를 <code class="docutils literal notranslate"><span class="pre">dst</span></code> 에 저장합니다.</li>
<li><code class="docutils literal notranslate"><span class="pre">dist.all_reduce(tensor,</span> <span class="pre">op,</span> <span class="pre">group)</span></code>: reduce와 같지만 결과는 모든 프로세스에
저장됩니다.</li>
<li><code class="docutils literal notranslate"><span class="pre">dist.scatter(tensor,</span> <span class="pre">src,</span> <span class="pre">scatter_list,</span> <span class="pre">group)</span></code>: <code class="docutils literal notranslate"><span class="pre">i번째</span> <span class="pre">tensor</span></code>
<code class="docutils literal notranslate"><span class="pre">scatter_list[i]</span></code> 를 <code class="docutils literal notranslate"><span class="pre">i번째</span></code> 프로세스에 복사합니다.</li>
<li><code class="docutils literal notranslate"><span class="pre">dist.gather(tensor,</span> <span class="pre">dst,</span> <span class="pre">gather_list,</span> <span class="pre">group)</span></code>: <code class="docutils literal notranslate"><span class="pre">dst</span></code> 의 모든 프로세스에서
<code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를 복사합니다</li>
<li><code class="docutils literal notranslate"><span class="pre">dist.all_gather(tensor_list,</span> <span class="pre">tensor,</span> <span class="pre">group)</span></code>:  모든 프로세스에서 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를
모든 프로세스의 <code class="docutils literal notranslate"><span class="pre">tensor_list</span></code> 에 복사합니다.</li>
</ul>
</div>
<div class="section" id="distributed-training">
<h2>분산 학습(Distributed Training)<a class="headerlink" href="#distributed-training" title="Permalink to this headline">¶</a></h2>
<!--
* Gloo Backend
* Simple all_reduce on the gradients
* Point to optimized DistributedDataParallel

TODO: Custom ring-allreduce
--><p><strong>알림:</strong> 이 섹션의 예제 스크립트를
<a class="reference external" href="https://github.com/seba-1511/dist_tuto.pth/">GitHub repository</a> 에서 찾으실
수 있습니다.</p>
<p>이제 분산 모듈이 어떻게 작동하는지 이해 했으므로 유용한 모듈을 작성해 보겠습니다.
우리의 목표는 <a class="reference external" href="http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel">DistributedDataParallel</a> 의
기능을 복제하는 것입니다. 물론, 이것은 교훈적인 예가 되지만, 실제 상황에서 위에
링크된 잘 검증되고 최적화 된 공식 버전을 사용해야합니다.</p>
<p>매우 간단하게 확률적 경사 하강법의 분산 버전을 구현하고자 합니다. 스크립트는 모든
프로세스가 데이터 배치에서 모델의 변화도를 계산한 다음 변화도를 평균합니다.
프로세스 수를 변경할 때 유사한 수렴 결과를 보장하기 위해 우선 데이터 세트를 분할해야
합니다. (아래 단편 코드 대신에
<a class="reference external" href="https://github.com/pytorch/tnt/blob/master/torchnet/dataset/splitdataset.py#L4">tnt.dataset.SplitDataset</a>
를 이용할 수 있습니다.)</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Dataset partitioning helper &quot;&quot;&quot;</span>
<span class="k">class</span> <span class="nc">Partition</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">data_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">DataPartitioner</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1234</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">partitions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">Random</span><span class="p">()</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">data_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">data_len</span><span class="p">)]</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">frac</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">:</span>
            <span class="n">part_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">frac</span> <span class="o">*</span> <span class="n">data_len</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">partitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">indexes</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">part_len</span><span class="p">])</span>
            <span class="n">indexes</span> <span class="o">=</span> <span class="n">indexes</span><span class="p">[</span><span class="n">part_len</span><span class="p">:]</span>

    <span class="k">def</span> <span class="nf">use</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Partition</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">partitions</span><span class="p">[</span><span class="n">partition</span><span class="p">])</span>
</pre></div>
</div>
<p>위의 단편 코드로 다음 몇 줄을 이용해 모든 데이터 세트를 간단하게 분할할 수 있습니다:</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Partitioning MNIST &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">partition_dataset</span><span class="p">():</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                                 <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                 <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                             <span class="p">]))</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
    <span class="n">bsz</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="n">partition_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">size</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">)]</span>
    <span class="n">partition</span> <span class="o">=</span> <span class="n">DataPartitioner</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">partition_sizes</span><span class="p">)</span>
    <span class="n">partition</span> <span class="o">=</span> <span class="n">partition</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">())</span>
    <span class="n">train_set</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">partition</span><span class="p">,</span>
                                         <span class="n">batch_size</span><span class="o">=</span><span class="n">bsz</span><span class="p">,</span>
                                         <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">bsz</span>
</pre></div>
</div>
<p>2개의 복제본이 있다고 가정하면, 각 프로세스는 60000 / 2 = 30000 샘플의
<code class="docutils literal notranslate"><span class="pre">train_set</span></code> 을 가질 것입니다. 또한 <strong>전체</strong> 배치 크기 128을 유지하기 위해 배치
크기를 복제본 수로 나눕니다.</p>
<p>이제는 일반적인 forward-backward-optimize 학습 코드를 작성하고, 모델의 변화도를
평균하는 함수 호출을 추가 할 수 있습니다. (다음은 공식
<a class="reference external" href="https://github.com/pytorch/examples/blob/master/mnist/main.py">PyTorch MNIST 예제</a>
에서 영감을 얻었습니다.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Distributed Synchronous SGD Example &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
        <span class="n">train_set</span><span class="p">,</span> <span class="n">bsz</span> <span class="o">=</span> <span class="n">partition_dataset</span><span class="p">()</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                              <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="n">num_batches</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_set</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">bsz</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_set</span><span class="p">:</span>
                <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
                <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">average_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(),</span> <span class="s1">&#39;, epoch &#39;</span><span class="p">,</span>
                  <span class="n">epoch</span><span class="p">,</span> <span class="s1">&#39;: &#39;</span><span class="p">,</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="n">num_batches</span><span class="p">)</span>
</pre></div>
</div>
<p>단순히 모델을 취하여 world의 변화도를 평균하는 <code class="docutils literal notranslate"><span class="pre">average_gradients</span> <span class="pre">(model)</span></code> 함수를
구현하는 것이 남았습니다.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Gradient averaging. &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">average_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">/=</span> <span class="n">size</span>
</pre></div>
</div>
<p><em>완성</em>! 우리는 분산 동기식 SGD를 성공적으로 구현했으며 대형 컴퓨터 클러스터에서
모든 모델을 학습 할 수 있었습니다.</p>
<p><strong>주의:</strong> 마지막 문장은 <em>기술적으로</em> 사실이지만 동기식 SGD의 상용 수준 구현하는데
필요한 더 많은 트릭이 있습니다. 다시말하면
<a class="reference external" href="http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel">검증되고 최적화된 함수</a> 를
사용하십시오.</p>
<div class="section" id="our-own-ring-allreduce">
<h3>Our Own Ring-Allreduce<a class="headerlink" href="#our-own-ring-allreduce" title="Permalink to this headline">¶</a></h3>
<p>추가 과제로서 DeepSpeech의 효율적인 ring allreduce 를 구현하고 싶다고 상상해보십시오.
이것은 지점간 집단 통신 (point-to-point collectives)을 사용하여 쉽게 구현됩니다.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Implementation of a ring-reduce with addition. &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">allreduce</span><span class="p">(</span><span class="n">send</span><span class="p">,</span> <span class="n">recv</span><span class="p">):</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
    <span class="n">send_buff</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">send</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">recv_buff</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">send</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">accum</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">send</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">accum</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">send</span><span class="p">[:]</span>

    <span class="n">left</span> <span class="o">=</span> <span class="p">((</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">size</span><span class="p">)</span> <span class="o">%</span> <span class="n">size</span>
    <span class="n">right</span> <span class="o">=</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">size</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Send send_buff</span>
            <span class="n">send_req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">send_buff</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">recv_buff</span><span class="p">,</span> <span class="n">left</span><span class="p">)</span>
            <span class="n">accum</span><span class="p">[:]</span> <span class="o">+=</span> <span class="n">recv</span><span class="p">[:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Send recv_buff</span>
            <span class="n">send_req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">recv_buff</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">send_buff</span><span class="p">,</span> <span class="n">left</span><span class="p">)</span>
            <span class="n">accum</span><span class="p">[:]</span> <span class="o">+=</span> <span class="n">send</span><span class="p">[:]</span>
        <span class="n">send_req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="n">recv</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">accum</span><span class="p">[:]</span>
</pre></div>
</div>
<p>위의 스크립트에서, <code class="docutils literal notranslate"><span class="pre">allreduce</span> <span class="pre">(send,</span> <span class="pre">recv)</span></code> 함수는 PyTorch에 있는 것과 약간 다른
특징을 가지고 있습니다.
그것은 <code class="docutils literal notranslate"><span class="pre">recv</span></code> tensor를 취해서 모든 <code class="docutils literal notranslate"><span class="pre">send</span></code> tensor의 합을 저장합니다. 독자에게
남겨진 실습으로, 우리의 버전과 DeepSpeech의 차이점은 여전히 한가지가 있습니다:
그들의 구현은 통신 대역폭을 최적으로 활용하기 위해 경사도 tensor를 <em>chunks</em> 로
나눕니다. (힌트:
<a class="reference external" href="http://pytorch.org/docs/master/torch.html#torch.chunk">toch.chunk</a>)</p>
</div>
</div>
<div class="section" id="advanced-topics">
<h2>Advanced Topics<a class="headerlink" href="#advanced-topics" title="Permalink to this headline">¶</a></h2>
<p>이제 <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> 보다 진보된 기능들을 발견 할 준비가 되었습니다. 커버할
부분이 많으므로 이 섹션은 두 개의 하위 섹션으로 구분됩니다:</p>
<ol class="arabic simple">
<li>통신 백엔드 : GPU-GPU 통신을 위해 MPI 및 Gloo를 사용하는 방법을 배웁니다.</li>
<li>초기화 방법 : <code class="docutils literal notranslate"><span class="pre">dist.init_process_group()</span></code> 에서 초기 구성 단계를 가장 잘
설정하는 방법을 이해합니다.</li>
</ol>
<div class="section" id="id2">
<h3>통신 백엔드<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> 의 가장 우아한 면 중 하나는 다른 백엔드 위에서 추상화하고
빌드 할 수 있는 능력입니다. 앞서 언급했듯이 현재 PyTorch에는 TCP, MPI 및 Gloo의
세 가지 백엔드가 구현되어 있습니다. 그것들은 원하는 사용 사례에 따라 서로 다른
특징과 trade-off 를 가지고 있습니다. 지원되는 기능의 비교표는
<a class="reference external" href="http://pytorch.org/docs/master/distributed.html#module-torch.distributed">여기</a>
에서 찾을 수 있습니다.</p>
<p><strong>TCP 백엔드</strong></p>
<p>지금까지 우리는 TCP 백엔드를 광범위하게 사용 해왔다. 그것은 대부분의 기계 및
운영체제에서 작동하도록 보장하기 때문에 개발 플랫폼으로 매우 편리합니다.
또한 CPU에서 모든 지점간 및 집단 통신 기능을 지원합니다. 그러나 GPU에 대한 지원은
없으며 통신 루틴이 MPI만큼 최적화되지 않았습니다.</p>
<p><strong>Gloo 백엔드</strong></p>
<p><a class="reference external" href="https://github.com/facebookincubator/gloo">Gloo 백엔드</a> 는 CPU와 GPU 모두를
위한 <em>집단 통신</em> 절차의 최적화된 구현을 제공합니다.
<a class="reference external" href="https://developer.nvidia.com/gpudirect">GPUDirect</a> 를 사용하여 CPU 메모리로
데이터를 전송하지 않고 통신을 수행 할 수 있기 때문에 GPU에서 특히 빛납니다.
또한 <a class="reference external" href="https://github.com/NVIDIA/nccl">NCCL</a> 을 사용하여 빠른 노드-내부
(intra-node) 통신을 수행 할 수 있으며 노드들-간(inter-node) 루틴을 위한
<a class="reference external" href="https://github.com/facebookincubator/gloo/blob/master/docs/algorithms.md">자체 알고리즘</a> 을
구현합니다.</p>
<p>버전 0.2.0부터, Gloo 백엔드는 PyTorch의 미리 컴파일 된 바이너리에 자동으로
포함됩니다. GPU에 <code class="docutils literal notranslate"><span class="pre">모델</span></code> 을 넣으면 배포된 SGD 예제가 제대로 작동하지 않습니다.
<code class="docutils literal notranslate"><span class="pre">init_processes</span> <span class="pre">(rank,</span> <span class="pre">size,</span> <span class="pre">fn,</span> <span class="pre">backend</span> <span class="pre">=</span> <span class="pre">'tcp')</span></code> 에서``backend = ‘gloo’`` 를
먼저 바꾸어서 고쳐 보겠습니다. 이 시점에서 스크립트는 여전히 CPU에서 실행되지만
백그라운드에서 Gloo 백엔드를 사용합니다. 여러 GPU를 사용하려면 다음과 같이
수정하십시오.</p>
<ol class="arabic simple" start="0">
<li><code class="docutils literal notranslate"><span class="pre">init_processes(rank,</span> <span class="pre">size,</span> <span class="pre">fn,</span> <span class="pre">backend='tcp')</span></code> =&gt;
<code class="docutils literal notranslate"><span class="pre">init_processes(rank,</span> <span class="pre">size,</span> <span class="pre">fn,</span> <span class="pre">backend='gloo')</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Net()</span></code> =&gt; <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Net().cuda(rank)</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">data,</span> <span class="pre">target</span> <span class="pre">=</span> <span class="pre">Variable(data),</span> <span class="pre">Variable(target)</span></code> =&gt;
<code class="docutils literal notranslate"><span class="pre">data,</span> <span class="pre">target</span> <span class="pre">=</span> <span class="pre">Variable(data.cuda(rank)),</span> <span class="pre">Variable(target.cuda(rank))</span></code></li>
</ol>
<p>위의 수정으로 우리 모델은 이제 2개의 GPU에서 학습하고, <code class="docutils literal notranslate"><span class="pre">watch</span> <span class="pre">nvidia-smi</span></code> 로
사용률을 모니터링 할 수 있습니다.</p>
<p><strong>MPI 백엔드</strong></p>
<p>MPI (Message Passing Interface)는 고성능 컴퓨팅 분야의 표준 도구입니다. 그것은
지점간과 집단 통신을 가능하게하고 <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> 의 API에 대한 주요
영감이었습니다. 다양한 목적으로 최적화된 여러 가지 MPI 구현 (예 :
<a class="reference external" href="https://www.open-mpi.org/">Open-MPI</a> , <a class="reference external" href="http://mvapich.cse.ohio-state.edu/">MVAPICH2</a> ,
<a class="reference external" href="https://software.intel.com/en-us/intel-mpi-library">Intel MPI</a> )이 있습니다.
MPI 백엔드를 사용하면 큰 컴퓨터 클러스터에서 MPI의 광범위한 가용성과 높은 수준의
최적화가 가능하다는 장점이 있습니다. <a class="reference external" href="https://developer.nvidia.com/mvapich">일부</a>
<a class="reference external" href="https://developer.nvidia.com/ibm-spectrum-mpi">최신</a>
<a class="reference external" href="http://www.open-mpi.org/">구현</a> 들은 CPU를 통한 메모리 복사를 피하기 위해서
CUDA IPC와 GPU 다이렉트 기술를 활용하고 있습니다.</p>
<p>불행하게도 PyTorch의 바이너리는 MPI 구현을 포함 할 수 없으므로 수동으로 다시
컴파일해야합니다. 다행히도, 이 컴파일 과정은 매우 간단합니다. PyTorch는 사용 가능한
MPI 구현을 자동으로 살펴볼 것입니다.
다음 단계는 PyTorch를 <a class="reference external" href="https://github.com/pytorch/pytorch#from-source">소스</a> 로
설치하여 MPI 백엔드를 설치합니다.</p>
<ol class="arabic simple">
<li>아나콘다 환경을 만들고 활성화하고, `
가이드 &lt;<a class="reference external" href="https://github.com/pytorch/pytorch#from-source">https://github.com/pytorch/pytorch#from-source</a>&gt;`__ 에 따라 모든 필수
조건을 설치하십시오. 그러나 아직 <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span></code> 을 실행하지
마십시오.</li>
<li>원하는 MPI 구현을 선택하고 설치하십시오. CUDA 인식하는 MPI를 활성화하려면
몇 가지 추가 단계가 필요할 수 있습니다. GPU <em>없이</em>  Open-MPI를 사용 할 것입니다:
<code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">-c</span> <span class="pre">conda-forge</span> <span class="pre">openmpi</span></code></li>
<li>이제 복제 된 PyTorch repo 로 이동하여 <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span></code> 을 실행하십시오.</li>
</ol>
<p>새로 설치된 백엔드를 테스트하려면 몇 가지 수정이 필요합니다.</p>
<ol class="arabic simple">
<li><code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">__name__</span> <span class="pre">==</span> <span class="pre">'__main__':</span></code> 아래 내용을 <code class="docutils literal notranslate"><span class="pre">init_processes(0,</span> <span class="pre">0,</span> <span class="pre">run,</span> <span class="pre">backend='mpi')</span></code> 로
변경하십시오.</li>
<li><code class="docutils literal notranslate"><span class="pre">mpirun</span> <span class="pre">-n</span> <span class="pre">4</span> <span class="pre">python</span> <span class="pre">myscript.py</span></code> 를 실행하십시오.</li>
</ol>
<p>이러한 변경의 이유는 MPI가 프로세스를 생성하기 전에 자체 환경을 만들어야하기 때문입니다.
MPI는 또한 자신의 프로세스를 생성하고 <code class="docutils literal notranslate"><span class="pre">init_process_group</span></code> 의 <code class="docutils literal notranslate"><span class="pre">rank</span></code> 와 <code class="docutils literal notranslate"><span class="pre">size</span></code> 인자를
불필요하게 만드는 <a class="reference external" href="#initialization-methods">초기화 방법</a> 에서 설명한 handshake 를
수행합니다. 각 프로세스의 계산 리소스를 맞추기 위해``mpirun``에 추가 인자를 전달할
수 있기 때문에 이것이 실제로 강력합니다.
(프로세스 당 코어 수, 특정 순위의 머신에 수동 할당,
<a class="reference external" href="https://www.open-mpi.org/faq/?category=running#mpirun-hostfile">기타 추가</a>
할 것들)
이렇게하면 다른 통신 백엔드와 같고 익숙한 출력을 얻어야합니다.</p>
</div>
<div class="section" id="id3">
<h3>초기화 방법<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>이 튜토리얼을 끝내기 위해, 호출한 첫 번째 함수인
<code class="docutils literal notranslate"><span class="pre">dist.init_process_group(backend,</span> <span class="pre">init_method)</span></code> 에 대해 이야기 해봅시다. 특히
각 프로세스 간의 초기 구성 단계를 담당하는 다양한 초기화 메소드를 살펴보겠습니다.
이러한 메서드를 사용하면 이 구성이 수행되는 방법을 정의 할 수 있습니다.
하드웨어 설정에 따라, 이러한 방법 중 하나는 자연스럽게 다른 것보다 더 적합해야
합니다. 다음 섹션들에 덧붙여
<a class="reference external" href="http://pytorch.org/docs/master/distributed.html#initialization">공식 문서</a> 를
살펴 봐야합니다.</p>
<p>초기화 메소드에 대해 배우기 전에, C/C++ 관점에서 <code class="docutils literal notranslate"><span class="pre">init_process_group</span></code> 뒤에
일어나는 것을 간단히 살펴 보겠습니다.</p>
<ol class="arabic simple">
<li>먼저, 인자가 구문 분석되고 유효성 검사가 수행됩니다.</li>
<li>백엔드는 <code class="docutils literal notranslate"><span class="pre">name2channel.at</span> <span class="pre">()</span></code> 함수를 통해 해결됩니다. <code class="docutils literal notranslate"><span class="pre">Channel</span></code> 클래스가
반환되고, 데이터 전송을 수행하는 데 사용됩니다.</li>
<li>GIL이 삭제되고, <code class="docutils literal notranslate"><span class="pre">THDProcessGroupInit</span> <span class="pre">()</span></code> 가 호출됩니다. 이것은 채널을
instantiates 하고 마스터 노드의 주소를 추가합니다.</li>
<li>순위 0의 프로세스는 <code class="docutils literal notranslate"><span class="pre">마스터</span></code> 단계를 실행하지만 다른 모든 순위는 <code class="docutils literal notranslate"><span class="pre">워커</span></code> 가
됩니다.</li>
<li>마스터<ol class="loweralpha">
<li>모든 워커를 위한 소켓을 생성합니다.</li>
<li>모든 워커가 연결되기를 기다립니다.</li>
<li>다른 프로세스의 위치에 대한 정보를 보냅니다.</li>
</ol>
</li>
<li>워커<ol class="loweralpha">
<li>마스터에 소켓을 생성합니다.</li>
<li>자신의 위치 정보를 보냅니다.</li>
<li>다른 워커에 대한 정보를 받습니다.</li>
<li>다른 모든 워커와 소켓을 열고 handshake를 합니다.</li>
</ol>
</li>
<li>초기화가 완료되고 모두가 모두와 연결됩니다.</li>
</ol>
<p><strong>환경 변수</strong></p>
<p>이 튜토리얼에서는 환경 변수 초기화 메소드를 사용해 왔습니다. 모든 머신에서 다음
네가지 환경 변수를 설정해서 모든 프로세스들이 마스터와 적합하게 연결될 수 있고
다른 프로세스의 정보를 얻고, 최종적으로 그들과 handshake 할 수 있습니다.</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code>: 순위 0의 프로세스를 호스트 할 머신의 자유 포트.</li>
<li><code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code>: 순위 0의 프로세스를 호스트 할 머신의 IP 주소.</li>
<li><code class="docutils literal notranslate"><span class="pre">WORLD_SIZE</span></code>: 기다려야하는 워커 숫자를 마스터가 알 수 있게하는 총 프로세스 수.</li>
<li><code class="docutils literal notranslate"><span class="pre">RANK</span></code>: 워커의 마스터 인지 아닌지를 알 수 있게 하는 각 프로세스의 순위.</li>
</ul>
<p><strong>공유 파일 시스템(Shared File System)</strong></p>
<p>공유 파일 시스템은 모든 프로세스가 공유 파일 시스템에 접속하는 것을 요구하며 공유
파일을 통해 이를 구성합니다. 이것은 각 프로세스가 파일을 열고, 정보를 쓰고, 모두가
그렇게 할 때까지 기다리는 것을 의미합니다. 필요한 모든 정보는 모든 프로세스에게
쉽게 사용 가능할 것입니다. 경쟁 조건을 피하기 위해 파일 시스템은
<a class="reference external" href="http://man7.org/linux/man-pages/man2/fcntl.2.html">fcntl</a> 을 통한 잠금을
지원해야합니다. 순위를 수동으로 지정하거나 프로세스가 스스로 순위를 매길 수
있습니다. 작업마다 고유한 <code class="docutils literal notranslate"><span class="pre">groupname</span></code> 을 정의하면, 여러 작업에 대해 동일한
파일 경로를 사용하고 충돌을 안전하게 피할 수 있습니다.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;file:///mnt/nfs/sharedfile&#39;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                        <span class="n">group_name</span><span class="o">=</span><span class="s1">&#39;mygroup&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>TCP 초기화 &amp; 멀티 캐스트</strong></p>
<p>TCP를 통한 초기화는 두 가지 방법으로 수행될 수 있습니다.:</p>
<ol class="arabic simple">
<li>순위 0 프로세스의 IP 주소와 worold의 크기를 제공.</li>
<li><em>어떤</em> 유효한 IP <a class="reference external" href="https://en.wikipedia.org/wiki/Multicast_address">멀티 캐스트 주소</a> 와
worold의 크기를 제공.</li>
</ol>
<p>첫 번째 경우 모든 워커는 순위 0의 프로세스에 연결할 수 있으며 위에서 설명한 절차를
따릅니다.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;tcp://10.1.1.20:23456&#39;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>두 번째 경우에, 멀티 캐스트 주소가 잠재적으로 활성화 될 수있는 노드 그룹을 지정하고
위 절차를 수행하기 전에 각 프로세스가 초기 handshake를 허용하여 구성을 처리 할 수
있습니다. 또한 TCP 멀티 캐스트 초기화는 동일한 클러스터에서 여러 작업을 스케줄 할
수 있도록 <code class="docutils literal notranslate"><span class="pre">group_name</span></code> 인자 (공유 파일 방법과 동일)를 지원합니다.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456&#39;</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<!--
## Internals
* init_process_group 뒤에 있는 마법 :

1. 인자의 유효성을 검사하고 구문을 분석합니다.
2. 백엔드 해결 : name2channel.at()
3. Drop GIL & THDProcessGroupInit : 채널을 인스턴스화하고 config의 마스터 주소를
   추가합니다.
4. 순위 0이 마스터, 다른 워커 초기화
5. 마스터 : 모든 워커를 위한 소켓 생성 -> 모든 워커가 연결될 때까지 대기 -> 다른
   프로세스의 위치에 대한 정보를 각자에게 보냄
6. 워커 : 마스터에 소켓을 생성하고, 자신의 정보를 보내고, 각 워커에 대한 정보를
   얻고, 각각과 handshake를 한다.
7. 이 때 모두가 모두와 handshake를 한다.
--><center><p><strong>알림</strong></p>
</center><p>PyTorch 개발자들이 구현, 문서화 및 테스트을 잘 수행해 준 것에 대해 감사드리고
싶습니다. 코드가 불분명 할 때, 나는 언제나 답을 찾기위해
<a class="reference external" href="http://pytorch.org/docs/master/distributed.html">docs</a> 나
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/test/test_distributed.py">tests</a> 의
도움을 받았습니다. 특히, 초기 초안에 대한 통찰력있는 의견 및 질문에 답변해주신
Soumith Chintala, Adam Paszke 및 Natalia Gimelshein에게 감사드립니다.</p>
</div>
</div>
</div>


           </div>
           <div class="articleComments">

           </div>
          </div>
          <footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">

        <a href="spatial_transformer_tutorial.html" class="btn btn-neutral float-right" title="Spatial Transformer Networks Tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>


        <a href="reinforcement_q_learning.html" class="btn btn-neutral" title="Reinforcement Learning (DQN) tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>

    </div>


  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, PyTorch (&amp; Korean translation is for its contributors).

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

</footer>

        </div>
      </div>

    </section>

  </div>





    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.3.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>





    <script type="text/javascript" src="../_static/js/theme.js"></script>




  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-71919972-2', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>