{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\uc2e0\uacbd\ub9dd(Neural Networks)\n=======================\n\n\uc2e0\uacbd\ub9dd\uc740 ``torch.nn`` \ud328\ud0a4\uc9c0\ub97c \uc774\uc6a9\ud558\uc5ec \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc9c0\uae08\uae4c\uc9c0 ``autograd`` \ub97c \uc0b4\ud3b4\ubd24\ub294\ub370\uc694, ``nn`` \uc740 \ubaa8\ub378\uc744 \uc815\uc758\ud558\uace0 \ubbf8\ubd84\ud558\ub294\ub370\n``autograd`` \ub97c \uc774\uc6a9\ud569\ub2c8\ub2e4.\n``nn.Module`` \uc740 \uacc4\uce35(layer)\uacfc ``output`` \uc744 \ubc18\ud658\ud558\ub294 ``forward(input)``\n\uba54\uc11c\ub4dc\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc22b\uc790 \uc774\ubbf8\uc9c0\ub97c \ubd84\ub958\ud558\ub294 \uc2e0\uacbd\ub9dd\uc744 \uc608\uc81c\ub85c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n\n.. figure:: /_static/img/mnist.png\n   :alt: convnet\n\n   convnet\n\n\uc774\ub294 \uac04\ub2e8\ud55c \ud53c\ub4dc-\ud3ec\uc6cc\ub4dc \ub124\ud2b8\uc6cc\ud06c(Feed-forward network)\uc785\ub2c8\ub2e4. \uc785\ub825(input)\uc744 \ubc1b\uc544\n\uc5ec\ub7ec \uacc4\uce35\uc5d0 \ucc28\ub840\ub85c \uc804\ub2ec\ud55c \ud6c4, \ucd5c\uc885 \ucd9c\ub825(output)\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\n\uc2e0\uacbd\ub9dd\uc758 \uc804\ud615\uc801\uc778 \ud559\uc2b5 \uacfc\uc815\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\n\n- \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218(\ub610\ub294 \uac00\uc911\uce58(weight))\ub97c \uac16\ub294 \uc2e0\uacbd\ub9dd\uc744 \uc815\uc758\ud569\ub2c8\ub2e4.\n- \ub370\uc774\ud130\uc14b(dataset) \uc785\ub825\uc744 \ubc18\ubcf5\ud569\ub2c8\ub2e4.\n- \uc785\ub825\uc744 \uc2e0\uacbd\ub9dd\uc5d0\uc11c \ucc98\ub9ac\ud569\ub2c8\ub2e4.\n- \uc190\uc2e4(loss; \ucd9c\ub825\uc774 \uc815\ub2f5\uc73c\ub85c\ubd80\ud130 \uc5bc\ub9c8\ub098 \ub5a8\uc5b4\uc838\uc788\ub294\uc9c0)\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n- \ubcc0\ud654\ub3c4(gradient)\ub97c \uc2e0\uacbd\ub9dd\uc758 \ub9e4\uac1c\ubcc0\uc218\ub4e4\uc5d0 \uc5ed\uc73c\ub85c \uc804\ud30c\ud569\ub2c8\ub2e4.\n- \uc2e0\uacbd\ub9dd\uc758 \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud569\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \ub2e4\uc74c\uacfc \uac19\uc740 \uac04\ub2e8\ud55c \uaddc\uce59\uc744 \uc774\uc6a9\ud569\ub2c8\ub2e4:\n  ``\uac00\uc911\uce58(wiehgt) = \uac00\uc911\uce58(weight) - \ud559\uc2b5\uc728(learning rate) * \ubcc0\ud654\ub3c4(gradient)``\n\n\uc2e0\uacbd\ub9dd \uc815\uc758\ud558\uae30\n---------------\n\n\uc2e0\uacbd\ub9dd\uc744 \uc815\uc758\ud569\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square you can only specify a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n\nnet = Net()\nprint(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``forward`` \ud568\uc218\ub9cc \uc815\uc758\ud558\uac8c \ub418\uba74, (\ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud558\ub294) ``backward`` \ud568\uc218\ub294\n``autograd`` \ub97c \uc774\uc6a9\ud558\uc5ec \uc790\ub3d9\uc73c\ub85c \uc815\uc758\ub429\ub2c8\ub2e4.\n``forward`` \ud568\uc218\uc5d0\uc11c\ub294 \uc5b4\ub5a0\ud55c Tensor \uc5f0\uc0b0\uc744 \uc774\uc6a9\ud574\ub3c4 \ub429\ub2c8\ub2e4.\n\n\ubaa8\ub378\uc758 \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218\ub4e4\uc740 ``net.parameters()`` \uc5d0 \uc758\ud574 \ubc18\ud658\ub429\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = list(net.parameters())\nprint(len(params))\nprint(params[0].size())  # conv1's .weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "forward\uc758 \uc785\ub825\uc740 ``autograd.Variable`` \uc774\uace0, \ucd9c\ub825 \ub610\ud55c \ub9c8\ucc2c\uac00\uc9c0\uc785\ub2c8\ub2e4.\nNote: \uc774 \uc2e0\uacbd\ub9dd(LeNet)\uc758 \uc785\ub825\uc740 32x32\uc785\ub2c8\ub2e4. \uc774 \uc2e0\uacbd\ub9dd\uc5d0 MNIST \ub370\uc774\ud130\uc14b\uc744\n\uc774\uc6a9\ud558\uae30 \uc704\ud574\uc11c\ub294, \ub370\uc774\ud130\uc14b\uc758 \uc774\ubbf8\uc9c0\ub97c 32x32\ub85c \ud06c\uae30\ub97c \ubcc0\uacbd\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input = Variable(torch.randn(1, 1, 32, 32))\nout = net(input)\nprint(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218\uc758 \ubcc0\ud654\ub3c4 \ubc84\ud37c(gradient buffer)\ub97c 0\uc73c\ub85c \uc124\uc815\ud558\uace0, \ubb34\uc791\uc704 \uac12\uc73c\ub85c \uc5ed\uc804\ud30c\ub97c \ud569\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net.zero_grad()\nout.backward(torch.randn(1, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` \uc740 \ubbf8\ub2c8 \ubc30\uce58(mini-batch)\ub9cc \uc9c0\uc6d0\ud569\ub2c8\ub2e4. ``torch.nn`` \ud328\ud0a4\uc9c0\n    \uc804\uccb4\ub294 \ud558\ub098\uc758 \uc0d8\ud50c\uc774 \uc544\ub2cc, \uc0d8\ud50c\ub4e4\uc758 \ubbf8\ub2c8\ubc30\uce58\ub9cc\uc744 \uc785\ub825\uc73c\ub85c \ubc1b\uc2b5\ub2c8\ub2e4.\n\n    \uc608\ub97c \ub4e4\uc5b4, ``nnConv2D`` \ub294 ``nSamples x nChannels x Height x Width`` \uc758\n    4\ucc28\uc6d0 Tensor\ub97c \uc785\ub825\uc73c\ub85c \ud569\ub2c8\ub2e4.\n\n    \ub9cc\uc57d \ud558\ub098\uc758 \uc0d8\ud50c\ub9cc \uc788\ub2e4\uba74, ``input.unsqueeze(0)`` \uc744 \uc0ac\uc6a9\ud574\uc11c \uac00\uc9dc \ucc28\uc6d0\uc744\n    \ucd94\uac00\ud569\ub2c8\ub2e4.</p></div>\n\n\n\uacc4\uc18d \uc9c4\ud589\ud558\uae30 \uc804\uc5d0, \uc9c0\uae08\uae4c\uc9c0 \uc0b4\ud3b4\ubd24\ub358 \uac83\ub4e4\uc744 \ub2e4\uc2dc \ud55c\ubc88 \uc694\uc57d\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n**Recap:**\n  -  ``torch.Tensor`` - *\ub2e4\ucc28\uc6d0 \ubc30\uc5f4*.\n  -  ``autograd.Variable`` - *Tensor\ub97c \uac10\uc2f8\uace0 \ubaa8\ub4e0 \uc5f0\uc0b0\uc744 \uae30\ub85d* \ud569\ub2c8\ub2e4.\n     ``Tensor`` \uc640 \ub3d9\uc77c\ud55c API\ub97c \uac16\uace0 \uc788\uc73c\uba70, ``backward()`` \uc640 \uac19\uc774 \ucd94\uac00\ub41c\n     \uac83\ub4e4\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c, tensor\uc5d0 \ub300\ud55c *\ubcc0\ud654\ub3c4\ub97c \uac16\uace0* \uc788\uc2b5\ub2c8\ub2e4.\n  -  ``nn.Module`` - \uc2e0\uacbd\ub9dd \ubaa8\ub4c8. *\ub9e4\uac1c\ubcc0\uc218\ub97c \ucea1\uc290\ud654(Encapsulation)\ud558\ub294 \uac04\ud3b8\ud55c\n     \ubc29\ubc95* \uc73c\ub85c, GPU\ub85c \uc774\ub3d9, \ub0b4\ubcf4\ub0b4\uae30(exporting), \ubd88\ub7ec\uc624\uae30(loading) \ub4f1\uc758 \uc791\uc5c5\uc744\n     \uc704\ud55c \ud5ec\ud37c(helper)\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.\n  -  ``nn.Parameter`` - \ubcc0\uc218\uc758 \ud55c \uc885\ub958\ub85c, ``Module`` *\uc5d0 \uc18d\uc131\uc73c\ub85c \ud560\ub2f9\ub420 \ub54c\n     \uc790\ub3d9\uc73c\ub85c \ub9e4\uac1c\ubcc0\uc218\ub85c \ub4f1\ub85d* \ub429\ub2c8\ub2e4.\n  -  ``autograd.Function`` - *autograd \uc5f0\uc0b0\uc758 \uc804\ubc29\ud5a5\uacfc \uc5ed\ubc29\ud5a5 \uc815\uc758* \ub97c \uad6c\ud604\ud569\ub2c8\ub2e4.\n     \ubaa8\ub4e0 ``Variable`` \uc5f0\uc0b0\uc740 \ud558\ub098 \uc774\uc0c1\uc758 ``Function`` \ub178\ub4dc\ub97c \uc0dd\uc131\ud558\uba70, \uac01 \ub178\ub4dc\ub294\n     ``Variable`` \uc744 \uc0dd\uc131\ud558\uace0 *\uc774\ub825(History)\uc744 \ubd80\ud638\ud654* \ud558\ub294 \ud568\uc218\ub4e4\uacfc \uc5f0\uacb0\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n**\uc774 \uc2dc\uc810\uc5d0 \uc6b0\ub9ac\uac00 \ub2e4\ub8ec \ub0b4\uc6a9\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:**\n  -  \uc2e0\uacbd\ub9dd\uc744 \uc815\uc758\ud558\ub294 \uac83\n  -  \uc785\ub825\uc744 \ucc98\ub9ac\ud558\uace0 ``backward`` \ub97c \ud638\ucd9c\ud558\ub294 \uac83\n\n**\ub354 \uc0b4\ud3b4\ubcfc \ub0b4\uc6a9\ub4e4\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:**\n  -  \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\ub294 \uac83\n  -  \uc2e0\uacbd\ub9dd\uc758 \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud558\ub294 \uac83\n\n\uc190\uc2e4 \ud568\uc218 (Loss Function)\n-------------------------\n\uc190\uc2e4 \ud568\uc218\ub294 (output, target)\uc744 \ud55c \uc30d(pair)\uc758 \uc785\ub825\uc73c\ub85c \ubc1b\uc544, output\uc774 target\uc73c\ub85c\ubd80\ud130\n\uc5bc\ub9c8\ub098 \ub5a8\uc5b4\uc838\uc788\ub294\uc9c0\ub97c \ucd94\uc815\ud558\ub294 \uac12\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n\nnn \ud328\ud0a4\uc9c0\uc5d0\ub294 \uc5ec\ub7ec\uac00\uc9c0\uc758 `\uc190\uc2e4 \ud568\uc218\ub4e4 <http://pytorch.org/docs/nn.html#loss-functions>`_\n\uc774 \uc874\uc7ac\ud569\ub2c8\ub2e4.\n\uac04\ub2e8\ud55c \uc190\uc2e4 \ud568\uc218\ub85c\ub294 \ucd9c\ub825\uacfc \ub300\uc0c1\uac04\uc758 \ud3c9\uade0\uc790\uc2b9\uc624\ucc28(mean-squared error)\ub97c \uacc4\uc0b0\ud558\ub294\n``nn.MSEloss`` \uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc608\ub97c \ub4e4\uba74:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "output = net(input)\ntarget = Variable(torch.arange(1, 11))  # a dummy target, for example\ntarget = target.view(1, -1)  # make it the same shape as output\ncriterion = nn.MSELoss()\n\nloss = criterion(output, target)\nprint(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c ``.grad_fn`` \uc18d\uc131\uc744 \uc774\uc6a9\ud558\uc5ec ``loss`` \ub97c \uc5ed\ubc29\ud5a5\uc5d0\uc11c \ub530\ub77c\uac00\ub2e4\ubcf4\uba74,\n\uc774\ub7ec\ud55c \ubaa8\uc2b5\uc758 \uc5f0\uc0b0 \uadf8\ub798\ud504\ub97c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n::\n\n    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n          -> view -> linear -> relu -> linear -> relu -> linear\n          -> MSELoss\n          -> loss\n\n\uc6b0\ub9ac\uac00 ``loss.backward()`` \ub97c \uc2e4\ud589\ud560 \ub54c, \uc804\uccb4 \uadf8\ub798\ud504\ub294 \uc190\uc2e4\uc5d0 \ub300\ud574 \ubbf8\ubd84\ub418\uba70,\n\uadf8\ub798\ud504 \ub0b4\uc758 \ubaa8\ub4e0 \ubcc0\uc218\ub294 \ubcc0\ud654\ub3c4\uac00 \ub204\uc801\ub41c ``.grad`` \ud568\uc218\ub97c \uac16\uac8c \ub429\ub2c8\ub2e4.\n\n\uba87 \ub2e8\uacc4\ub9cc \ub530\ub77c\uac00\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(loss.grad_fn)  # MSELoss\nprint(loss.grad_fn.next_functions[0][0])  # Linear\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc5ed\uc804\ud30c(Backprop)\n----------------\n\uc624\ucc28(error)\ub97c \uc5ed\uc804\ud30c\ud558\uae30 \uc704\ud574 \ud560 \uc77c\uc740 ``loss.backward()`` \uc774 \uc804\ubd80\uc785\ub2c8\ub2e4.\n\uae30\uc874 \ubcc0\ud654\ub3c4\ub97c \uc9c0\uc6b0\ub294 \uc791\uc5c5\uc774 \ud544\uc694\ud55c\ub370, \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 \ubcc0\ud654\ub3c4\uac00 \uae30\uc874\uc758 \uac83\uc5d0\n\ub204\uc801\ub418\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n\n\n\uc774\uc81c ``loss.backward()`` \ub97c \ud638\ucd9c\ud558\uc5ec \uc5ed\uc804\ud30c \uc804\uacfc \ud6c4\uc5d0 conv1\uc758 bias gradient\ub97c\n\uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n\nprint('conv1.bias.grad before backward')\nprint(net.conv1.bias.grad)\n\nloss.backward()\n\nprint('conv1.bias.grad after backward')\nprint(net.conv1.bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc9c0\uae08\uae4c\uc9c0 \uc190\uc2e4 \ud568\uc218\ub97c \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub294\uc9c0\ub97c \uc0b4\ud3b4\ubd24\uc2b5\ub2c8\ub2e4.\n\n**\ub354 \uc77d\uc5b4\ubcf4\uae30:**\n\n  \uc2e0\uacbd\ub9dd \ud328\ud0a4\uc9c0(neural network package)\uc5d0\ub294 \uc2ec\uce35 \uc2e0\uacbd\ub9dd(deep neural network)\uc744\n  \uad6c\uc131\ud558\ub294 \ub2e4\uc591\ud55c \ubaa8\ub4c8\uacfc \uc190\uc2e4\ud568\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n  \uc804\uccb4 \ubaa9\ub85d\uc740 `\uc774 \ubb38\uc11c <http://pytorch.org/docs/nn>`_ \uc5d0 \uc788\uc2b5\ub2c8\ub2e4.\n\n**\uc774\uc81c \ub354 \uc0b4\ud3b4\ubcfc \ub0b4\uc6a9\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:**\n\n  -  \uc2e0\uacbd\ub9dd\uc758 \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud558\ub294 \uac83\n\n\uac00\uc911\uce58 \uac31\uc2e0\n------------------\n\uc2e4\uc81c\ub85c \ub9ce\uc774 \uc0ac\uc6a9\ub418\ub294 \uac00\uc7a5 \ub2e8\uc21c\ud55c \uac31\uc2e0 \uaddc\uce59\uc740 \ud655\ub960\uc801 \uacbd\uc0ac\ud558\uac15\ubc95(SGD; Stochastic\nGradient Descent)\uc785\ub2c8\ub2e4:\n\n     ``\uac00\uc911\uce58(wiehgt) = \uac00\uc911\uce58(weight) - \ud559\uc2b5\uc728(learning rate) * \ubcc0\ud654\ub3c4(gradient)``\n\n\uac04\ub2e8\ud55c Python \ucf54\ub4dc\ub85c \uc774\ub97c \uad6c\ud604\ud574\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n.. code:: python\n\n    learning_rate = 0.01\n    for f in net.parameters():\n        f.data.sub_(f.grad.data * learning_rate)\n\n\uadf8\ub7ec\ub098, \uc2e0\uacbd\ub9dd\uc744 \uad6c\uc131\ud560 \ub54c SGD, Nesterov-SGD, Adam, RMSProp \ub4f1\uacfc \uac19\uc740 \ub2e4\uc591\ud55c\n\uac31\uc2e0 \uaddc\uce59\uc744 \uc0ac\uc6a9\ud558\uace0 \uc2f6\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574\uc11c ``torch.optim`` \ub77c\ub294 \uc791\uc740\n\ud328\ud0a4\uc9c0\uc5d0 \uc774\ub7ec\ud55c \ubc29\ubc95\ub4e4\uc744 \ubaa8\ub450 \uad6c\ud604\ud574\ub450\uc5c8\uc2b5\ub2c8\ub2e4. \uc0ac\uc6a9\ubc95\uc740 \ub9e4\uc6b0 \uac04\ub2e8\ud569\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n\n# Optimizer\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\n# \ud559\uc2b5 \uacfc\uc815(training loop)\uc5d0\uc11c\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\noptimizer.zero_grad()   # zero the gradient buffers\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()    # Does the update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n\n      ``optimizer.zero_grad()`` \ub97c \uc0ac\uc6a9\ud558\uc5ec \uc218\ub3d9\uc73c\ub85c \ubcc0\ud654\ub3c4 \ubc84\ud37c\ub97c 0\uc73c\ub85c \uc124\uc815\ud558\ub294\n      \uac83\uc5d0 \uc720\uc758\ud558\uc138\uc694. \uc774\ub294 `\uc5ed\uc804\ud30c(Backprop)`_ \uc139\uc158\uc5d0\uc11c \uc124\uba85\ud55c \uac83\ucc98\ub7fc \ubcc0\ud654\ub3c4\uac00\n      \ub204\uc801\ub418\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}